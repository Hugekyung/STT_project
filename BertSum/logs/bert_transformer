[2020-10-27 01:22:12,493 INFO] Device ID -1
[2020-10-27 01:22:12,495 INFO] Device cpu
[2020-10-27 01:22:13,382 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz not found in cache, downloading to C:\Users\USER\AppData\Local\Temp\tmpadmeoujx
[2020-10-27 01:28:11,198 INFO] copying C:\Users\USER\AppData\Local\Temp\tmpadmeoujx to cache at ../temp\437da855f7aeb6dcc47ee03b11ac55bfbc069d31354f6867f3b298aad8429925.dd2dce7e7331017693bd2230dbc8015b12a975201a420a856a6efbf7ae9d84c5
[2020-10-27 01:28:11,886 INFO] creating metadata file for ../temp\437da855f7aeb6dcc47ee03b11ac55bfbc069d31354f6867f3b298aad8429925.dd2dce7e7331017693bd2230dbc8015b12a975201a420a856a6efbf7ae9d84c5
[2020-10-27 01:28:11,890 INFO] removing temp file C:\Users\USER\AppData\Local\Temp\tmpadmeoujx
[2020-10-27 01:28:11,966 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz from cache at ../temp\437da855f7aeb6dcc47ee03b11ac55bfbc069d31354f6867f3b298aad8429925.dd2dce7e7331017693bd2230dbc8015b12a975201a420a856a6efbf7ae9d84c5
[2020-10-27 01:28:11,967 INFO] extracting archive file ../temp\437da855f7aeb6dcc47ee03b11ac55bfbc069d31354f6867f3b298aad8429925.dd2dce7e7331017693bd2230dbc8015b12a975201a420a856a6efbf7ae9d84c5 to temp dir C:\Users\USER\AppData\Local\Temp\tmpcr1j54_9
[2020-10-27 01:28:17,287 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

[2020-10-27 01:28:19,537 INFO] Summarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(105879, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): BertLayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (encoder): TransformerInterEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2020-10-27 01:28:19,644 INFO] * number of parameters: 173665025
[2020-10-27 01:28:19,645 INFO] Start training...
[2020-10-27 01:28:19,649 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:31:15,063 INFO] Device ID -1
[2020-10-27 01:31:15,063 INFO] Device cpu
[2020-10-27 01:31:15,962 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz from cache at ../temp\437da855f7aeb6dcc47ee03b11ac55bfbc069d31354f6867f3b298aad8429925.dd2dce7e7331017693bd2230dbc8015b12a975201a420a856a6efbf7ae9d84c5
[2020-10-27 01:31:15,972 INFO] extracting archive file ../temp\437da855f7aeb6dcc47ee03b11ac55bfbc069d31354f6867f3b298aad8429925.dd2dce7e7331017693bd2230dbc8015b12a975201a420a856a6efbf7ae9d84c5 to temp dir C:\Users\USER\AppData\Local\Temp\tmpjr1hkc7w
[2020-10-27 01:31:20,835 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

[2020-10-27 01:31:22,886 INFO] Summarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(105879, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): BertLayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (encoder): TransformerInterEncoder(
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_inter): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (wo): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2020-10-27 01:31:22,942 INFO] * number of parameters: 173665025
[2020-10-27 01:31:22,946 INFO] Start training...
[2020-10-27 01:31:22,949 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:31:26,303 INFO] Step  1/  100; xent: 3.33; lr: 1.0000000;   0 docs/s;      3 sec
[2020-10-27 01:31:29,646 INFO] Step  2/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;      7 sec
[2020-10-27 01:31:33,000 INFO] Step  3/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;     10 sec
[2020-10-27 01:31:36,582 INFO] Step  4/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;     14 sec
[2020-10-27 01:31:40,234 INFO] Step  5/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;     17 sec
[2020-10-27 01:31:40,236 INFO] Saving checkpoint ../models/bert_transformer\model_step_5.pt
[2020-10-27 01:31:42,973 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:31:46,600 INFO] Step  6/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;     24 sec
[2020-10-27 01:31:50,402 INFO] Step  7/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;     27 sec
[2020-10-27 01:31:53,643 INFO] Step  8/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;     31 sec
[2020-10-27 01:31:57,311 INFO] Step  9/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;     34 sec
[2020-10-27 01:32:01,012 INFO] Step 10/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;     38 sec
[2020-10-27 01:32:01,021 INFO] Saving checkpoint ../models/bert_transformer\model_step_10.pt
[2020-10-27 01:32:03,590 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:32:06,857 INFO] Step 11/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;     44 sec
[2020-10-27 01:32:10,734 INFO] Step 12/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;     48 sec
[2020-10-27 01:32:14,718 INFO] Step 13/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;     52 sec
[2020-10-27 01:32:18,574 INFO] Step 14/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;     56 sec
[2020-10-27 01:32:22,260 INFO] Step 15/  100; xent: 600.00; lr: 1.0000000;   0 docs/s;     59 sec
[2020-10-27 01:32:22,263 INFO] Saving checkpoint ../models/bert_transformer\model_step_15.pt
[2020-10-27 01:32:24,950 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:32:28,461 INFO] Step 16/  100; xent: 800.00; lr: 1.0000000;   0 docs/s;     66 sec
[2020-10-27 01:32:32,546 INFO] Step 17/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;     70 sec
[2020-10-27 01:32:36,511 INFO] Step 18/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;     74 sec
[2020-10-27 01:32:40,909 INFO] Step 19/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;     78 sec
[2020-10-27 01:32:44,903 INFO] Step 20/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;     82 sec
[2020-10-27 01:32:44,905 INFO] Saving checkpoint ../models/bert_transformer\model_step_20.pt
[2020-10-27 01:32:47,719 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:32:51,275 INFO] Step 21/  100; xent: 600.00; lr: 1.0000000;   0 docs/s;     88 sec
[2020-10-27 01:32:55,689 INFO] Step 22/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;     93 sec
[2020-10-27 01:32:59,828 INFO] Step 23/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;     97 sec
[2020-10-27 01:33:04,024 INFO] Step 24/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;    101 sec
[2020-10-27 01:33:08,248 INFO] Step 25/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    105 sec
[2020-10-27 01:33:08,259 INFO] Saving checkpoint ../models/bert_transformer\model_step_25.pt
[2020-10-27 01:33:10,981 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:33:14,882 INFO] Step 26/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    112 sec
[2020-10-27 01:33:18,832 INFO] Step 27/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    116 sec
[2020-10-27 01:33:23,107 INFO] Step 28/  100; xent: 800.00; lr: 1.0000000;   0 docs/s;    120 sec
[2020-10-27 01:33:27,321 INFO] Step 29/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    124 sec
[2020-10-27 01:33:31,488 INFO] Step 30/  100; xent: 800.00; lr: 1.0000000;   0 docs/s;    129 sec
[2020-10-27 01:33:31,492 INFO] Saving checkpoint ../models/bert_transformer\model_step_30.pt
[2020-10-27 01:33:34,311 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:33:38,133 INFO] Step 31/  100; xent: 800.00; lr: 1.0000000;   0 docs/s;    135 sec
[2020-10-27 01:33:42,126 INFO] Step 32/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    139 sec
[2020-10-27 01:33:46,775 INFO] Step 33/  100; xent: 600.00; lr: 1.0000000;   0 docs/s;    144 sec
[2020-10-27 01:33:51,008 INFO] Step 34/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    148 sec
[2020-10-27 01:33:55,136 INFO] Step 35/  100; xent: 900.00; lr: 1.0000000;   0 docs/s;    152 sec
[2020-10-27 01:33:55,139 INFO] Saving checkpoint ../models/bert_transformer\model_step_35.pt
[2020-10-27 01:33:57,825 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:34:01,845 INFO] Step 36/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    159 sec
[2020-10-27 01:34:06,098 INFO] Step 37/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    163 sec
[2020-10-27 01:34:10,236 INFO] Step 38/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    167 sec
[2020-10-27 01:34:14,484 INFO] Step 39/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    172 sec
[2020-10-27 01:34:18,640 INFO] Step 40/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    176 sec
[2020-10-27 01:34:18,645 INFO] Saving checkpoint ../models/bert_transformer\model_step_40.pt
[2020-10-27 01:34:21,333 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:34:25,282 INFO] Step 41/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    182 sec
[2020-10-27 01:34:29,649 INFO] Step 42/  100; xent: 1200.00; lr: 1.0000000;   0 docs/s;    187 sec
[2020-10-27 01:34:33,875 INFO] Step 43/  100; xent: 600.00; lr: 1.0000000;   0 docs/s;    191 sec
[2020-10-27 01:34:37,933 INFO] Step 44/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;    195 sec
[2020-10-27 01:34:42,297 INFO] Step 45/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    199 sec
[2020-10-27 01:34:42,308 INFO] Saving checkpoint ../models/bert_transformer\model_step_45.pt
[2020-10-27 01:34:45,027 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:34:48,939 INFO] Step 46/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    206 sec
[2020-10-27 01:34:53,262 INFO] Step 47/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    210 sec
[2020-10-27 01:34:57,402 INFO] Step 48/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    214 sec
[2020-10-27 01:35:01,751 INFO] Step 49/  100; xent: 0.00; lr: 1.0000000;   0 docs/s;    219 sec
[2020-10-27 01:35:06,298 INFO] Step 50/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    223 sec
[2020-10-27 01:35:06,300 INFO] Saving checkpoint ../models/bert_transformer\model_step_50.pt
[2020-10-27 01:35:09,081 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:35:13,199 INFO] Step 51/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    230 sec
[2020-10-27 01:35:17,535 INFO] Step 52/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    235 sec
[2020-10-27 01:35:22,120 INFO] Step 53/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    239 sec
[2020-10-27 01:35:26,184 INFO] Step 54/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;    243 sec
[2020-10-27 01:35:30,514 INFO] Step 55/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    248 sec
[2020-10-27 01:35:30,516 INFO] Saving checkpoint ../models/bert_transformer\model_step_55.pt
[2020-10-27 01:35:33,235 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:35:36,559 INFO] Step 56/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    254 sec
[2020-10-27 01:35:40,521 INFO] Step 57/  100; xent: 900.00; lr: 1.0000000;   0 docs/s;    258 sec
[2020-10-27 01:35:44,241 INFO] Step 58/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    261 sec
[2020-10-27 01:35:48,064 INFO] Step 59/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    265 sec
[2020-10-27 01:35:51,726 INFO] Step 60/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    269 sec
[2020-10-27 01:35:51,728 INFO] Saving checkpoint ../models/bert_transformer\model_step_60.pt
[2020-10-27 01:36:01,030 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:36:04,709 INFO] Step 61/  100; xent: 800.00; lr: 1.0000000;   0 docs/s;    282 sec
[2020-10-27 01:36:08,633 INFO] Step 62/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    286 sec
[2020-10-27 01:36:13,045 INFO] Step 63/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    290 sec
[2020-10-27 01:36:17,303 INFO] Step 64/  100; xent: 800.00; lr: 1.0000000;   0 docs/s;    294 sec
[2020-10-27 01:36:21,820 INFO] Step 65/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;    299 sec
[2020-10-27 01:36:21,822 INFO] Saving checkpoint ../models/bert_transformer\model_step_65.pt
[2020-10-27 01:36:34,770 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:36:39,197 INFO] Step 66/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    316 sec
[2020-10-27 01:36:43,448 INFO] Step 67/  100; xent: 600.00; lr: 1.0000000;   0 docs/s;    320 sec
[2020-10-27 01:36:47,792 INFO] Step 68/  100; xent: 1000.00; lr: 1.0000000;   0 docs/s;    325 sec
[2020-10-27 01:36:52,356 INFO] Step 69/  100; xent: 900.00; lr: 1.0000000;   0 docs/s;    329 sec
[2020-10-27 01:36:56,791 INFO] Step 70/  100; xent: 0.00; lr: 1.0000000;   0 docs/s;    334 sec
[2020-10-27 01:36:56,794 INFO] Saving checkpoint ../models/bert_transformer\model_step_70.pt
[2020-10-27 01:37:13,720 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:37:18,158 INFO] Step 71/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    355 sec
[2020-10-27 01:37:22,676 INFO] Step 72/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    360 sec
[2020-10-27 01:37:27,133 INFO] Step 73/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    364 sec
[2020-10-27 01:37:31,556 INFO] Step 74/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    369 sec
[2020-10-27 01:37:35,831 INFO] Step 75/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    373 sec
[2020-10-27 01:37:35,833 INFO] Saving checkpoint ../models/bert_transformer\model_step_75.pt
[2020-10-27 01:37:44,159 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:37:48,517 INFO] Step 76/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    386 sec
[2020-10-27 01:37:52,847 INFO] Step 77/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    390 sec
[2020-10-27 01:37:57,205 INFO] Step 78/  100; xent: 1100.00; lr: 1.0000000;   0 docs/s;    394 sec
[2020-10-27 01:38:01,637 INFO] Step 79/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    399 sec
[2020-10-27 01:38:06,006 INFO] Step 80/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    403 sec
[2020-10-27 01:38:06,009 INFO] Saving checkpoint ../models/bert_transformer\model_step_80.pt
[2020-10-27 01:38:08,794 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:38:13,215 INFO] Step 81/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    410 sec
[2020-10-27 01:38:17,711 INFO] Step 82/  100; xent: 1100.00; lr: 1.0000000;   0 docs/s;    415 sec
[2020-10-27 01:38:22,066 INFO] Step 83/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    419 sec
[2020-10-27 01:38:26,449 INFO] Step 84/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    423 sec
[2020-10-27 01:38:30,876 INFO] Step 85/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;    428 sec
[2020-10-27 01:38:30,879 INFO] Saving checkpoint ../models/bert_transformer\model_step_85.pt
[2020-10-27 01:38:33,632 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:38:38,088 INFO] Step 86/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    435 sec
[2020-10-27 01:38:42,631 INFO] Step 87/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    440 sec
[2020-10-27 01:38:47,049 INFO] Step 88/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    444 sec
[2020-10-27 01:38:51,806 INFO] Step 89/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;    449 sec
[2020-10-27 01:38:56,444 INFO] Step 90/  100; xent: 200.00; lr: 1.0000000;   0 docs/s;    453 sec
[2020-10-27 01:38:56,446 INFO] Saving checkpoint ../models/bert_transformer\model_step_90.pt
[2020-10-27 01:38:59,368 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:39:03,886 INFO] Step 91/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    461 sec
[2020-10-27 01:39:08,366 INFO] Step 92/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    465 sec
[2020-10-27 01:39:13,078 INFO] Step 93/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    470 sec
[2020-10-27 01:39:17,867 INFO] Step 94/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    475 sec
[2020-10-27 01:39:22,509 INFO] Step 95/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    480 sec
[2020-10-27 01:39:22,512 INFO] Saving checkpoint ../models/bert_transformer\model_step_95.pt
[2020-10-27 01:39:32,376 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:39:36,865 INFO] Step 96/  100; xent: 300.00; lr: 1.0000000;   0 docs/s;    494 sec
[2020-10-27 01:39:41,414 INFO] Step 97/  100; xent: 400.00; lr: 1.0000000;   0 docs/s;    498 sec
[2020-10-27 01:39:45,986 INFO] Step 98/  100; xent: 700.00; lr: 1.0000000;   0 docs/s;    503 sec
[2020-10-27 01:39:50,478 INFO] Step 99/  100; xent: 500.00; lr: 1.0000000;   0 docs/s;    508 sec
[2020-10-27 01:39:54,916 INFO] Step 100/  100; xent: 100.00; lr: 1.0000000;   0 docs/s;    512 sec
[2020-10-27 01:39:54,918 INFO] Saving checkpoint ../models/bert_transformer\model_step_100.pt
[2020-10-27 01:40:29,021 INFO] Loading train dataset from ../bert_data\ndm_sample.train.0.bert.pt, number of examples: 5
[2020-10-27 01:43:38,598 INFO] Loading checkpoint from ../models/bert_transformer/model_step_100.pt
[2020-10-27 01:43:40,749 INFO] Loading test dataset from ../bert_data\ndm_sample.test.0.bert.pt, number of examples: 1
[2020-10-27 01:43:40,758 INFO] * number of parameters: 173665025
[2020-10-27 01:43:41,674 INFO] Validation xent: 100 at step 100
