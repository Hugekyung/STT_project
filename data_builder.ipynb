{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    return x.replace(\"(\",\"\").replace(\")\",\"\").replace(\"{\",\"\").replace(\"}\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace('\"',\"\").replace(\"'\",\"\").replace(\"=\",\"\")\n",
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    # words = _split_into_words(sentences)\n",
    "\n",
    "    words = sum(sentences, [])\n",
    "    # words = [w for w in words if w not in stopwords]\n",
    "    return _get_ngrams(n, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1=\"https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=015&aid=0004435156&date=20201021&type=1&rankingSeq=1&rankingSectionId=104\"\n",
    "url2=\"https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=296&aid=0000046738&date=20201011&type=1&rankingSeq=2&rankingSectionId=103\"\n",
    "url3=\"https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=009&aid=0004678970&date=20201021&type=1&rankingSeq=5&rankingSectionId=103\"\n",
    "url4=\"https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=025&aid=0003045159&date=20201021&type=1&rankingSeq=1&rankingSectionId=105\"\n",
    "url5=\"https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=138&aid=0002093413&date=20201021&type=1&rankingSeq=2&rankingSectionId=105\"\n",
    "url6=\"https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=056&aid=0010920303&date=20201021&type=1&rankingSeq=3&rankingSectionId=105\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article=Article(url1, language='ko')\n",
    "article.download()\n",
    "article.parse()\n",
    "text1=article.text\n",
    "article=Article(url2, language='ko')\n",
    "article.download()\n",
    "article.parse()\n",
    "text2=article.text\n",
    "article=Article(url3, language='ko')\n",
    "article.download()\n",
    "article.parse()\n",
    "text3=article.text\n",
    "article=Article(url4, language='ko')\n",
    "article.download()\n",
    "article.parse()\n",
    "text4=article.text\n",
    "article=Article(url5, language='ko')\n",
    "article.download()\n",
    "article.parse()\n",
    "text5=article.text\n",
    "article=Article(url6, language='ko')\n",
    "article.download()\n",
    "article.parse()\n",
    "text6=article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_text1=\"그런데 왓츠앱의 경우 암호화 처리를 하기 때문에 왓츠앱을 통해 주고받은 대화 내용을 은행에서 확인하기가 쉽지 않다는 점이 문제가 돼 왔다. 그러나 은행 직원들은 왓츠앱을 통해 주변 사람들과 편하게 대화를 나눌 수 있기 때문에 회사 방침에도 불구하고 사용을 전면 중단하지는 않고 있다. 모간스탠리가 이번에 해고한 킹 대표 등 두 명 모두 왓츠앱을 통해 위법행위를 저지르지 않은 것으로 드러났다.\"\n",
    "tgt_text2=\"건강수명을 누리려면 먼저 치매, 만성질환, 암 등 치료가 어렵고 투병기간이 긴 질병부터 예방해야 한다. 근육을 저축하고 키우기 위해서는 근력운동과 단백질 섭취, 비타민 D 섭취 등을 동시에 하는 것이 가장 좋다. 젊을 때부터 근력운동을 통해 근육의 힘을 많이 키워놓으면 크게 도움이 된다.\"\n",
    "tgt_text3=\"롯데그룹이 갈수록 치열해지는 온·오프라인 유통시장 경쟁에서 승리하기 위해 본격적인 빅데이터 경영에 나선다. 데이터를 무기로 시장을 잠식하는 네이버와 카카오 등 IT공룡들의 공세에 맞서 강희태 유통BU장·롯데쇼핑 부회장 직속의 빅데이터 조직을 발족, 그간 각 계열사별로 따로 관리하던 소비데이터를 그룹 차원에서 한데 모아 분석하고 이에 맞는 맞춤형 쇼핑서비스를 선보인다는 목표다. 21일 롯데에 따르면 강 부회장이 이끄는 롯데 유통BU는 지난 1일 강 부회장 직속의 데이터 거버넌스 태스크포스를 출범하고, TF장이자 CDO로 윤영선 롯데정보통신 상무를 임명했다.\"\n",
    "tgt_text4=\"8년간 의식장애로 인한 마비로 침대에 누워있거나 휠체어에 묶여 있던 네덜란드 30대 남성이 수면제 복용 20분 만에 정상능력을 회복해 다시 걷고 말하고, 먹을 수 있게 됐다. 한편 앞서도 혼수상태에 있던 환자가 수면제 복용 뒤 일시적으로 정상 능력을 회복했다는 연구결과가 있었다. 연구팀은 리처드 치료를 계기로 수면제를 활용해 뇌손상이나 혼수상태에 있는 환자를 영구적인 정상 상태로 회복시키는 방안을 연구하고 있다.\"\n",
    "tgt_text5=\"예상보다 글로벌 가입자가 적게 늘었다. 대신 한국과 일본이 체면치레를 해줬다. 넷플릭스가 이번 분기 국내에서 상당한 수익을 벌고 갈 것으로 예상되면서 국회를 중심으로 한국시장 무임승차 논란이 다시 한번 불거질 전망이다.\"\n",
    "tgt_text6=\"미국의 소행성 탐사선 '오시리스-렉스'가 20일 소행성 '101955 베누' 표면에 접지해 암석 표본을 채집하는 데 성공했습니다. 이번에 채취된 베누의 샘플은 2023년쯤 받아볼 수 있을 전망입니다. 나사는 이를 통해 베누의 구성 성분과 기원, 형성과정 등을 살피고 수분의 존재 여부 등도 조사하게 됩니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>사진=연합뉴스 사진=연합뉴스\\n\\n미국 월스트리트에서 거액 연봉을 받던 트레이더들이...</td>\n",
       "      <td>그런데 왓츠앱의 경우 암호화 처리를 하기 때문에 왓츠앱을 통해 주고받은 대화 내용을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[사진=게티이미지뱅크] [사진=게티이미지뱅크]\\n\\n요즘은 \"오래사세요\"(장수)보다...</td>\n",
       "      <td>건강수명을 누리려면 먼저 치매, 만성질환, 암 등 치료가 어렵고 투병기간이 긴 질병...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>롯데그룹이 갈수록 치열해지는 온·오프라인 유통시장 경쟁에서 승리하기 위해 본격적인 ...</td>\n",
       "      <td>롯데그룹이 갈수록 치열해지는 온·오프라인 유통시장 경쟁에서 승리하기 위해 본격적인 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>졸피뎀. 연합뉴스 졸피뎀. 연합뉴스\\n\\n8년간 의식장애로 인한 마비로 침대에 누워...</td>\n",
       "      <td>8년간 의식장애로 인한 마비로 침대에 누워있거나 휠체어에 묶여 있던 네덜란드 30대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[디지털데일리 권하영기자] 넷플릭스가 기대이하 성적을 냈다. 예상보다 글로벌 가입자...</td>\n",
       "      <td>예상보다 글로벌 가입자가 적게 늘었다. 대신 한국과 일본이 체면치레를 해줬다. 넷플...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>사진출처: NASA 사진출처: NASA\\n\\n사진출처: NASA 사진출처: NASA...</td>\n",
       "      <td>미국의 소행성 탐사선 '오시리스-렉스'가 20일 소행성 '101955 베누' 표면에...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  사진=연합뉴스 사진=연합뉴스\\n\\n미국 월스트리트에서 거액 연봉을 받던 트레이더들이...   \n",
       "1  [사진=게티이미지뱅크] [사진=게티이미지뱅크]\\n\\n요즘은 \"오래사세요\"(장수)보다...   \n",
       "2  롯데그룹이 갈수록 치열해지는 온·오프라인 유통시장 경쟁에서 승리하기 위해 본격적인 ...   \n",
       "3  졸피뎀. 연합뉴스 졸피뎀. 연합뉴스\\n\\n8년간 의식장애로 인한 마비로 침대에 누워...   \n",
       "4  [디지털데일리 권하영기자] 넷플릭스가 기대이하 성적을 냈다. 예상보다 글로벌 가입자...   \n",
       "5  사진출처: NASA 사진출처: NASA\\n\\n사진출처: NASA 사진출처: NASA...   \n",
       "\n",
       "                                                 tgt  \n",
       "0  그런데 왓츠앱의 경우 암호화 처리를 하기 때문에 왓츠앱을 통해 주고받은 대화 내용을...  \n",
       "1  건강수명을 누리려면 먼저 치매, 만성질환, 암 등 치료가 어렵고 투병기간이 긴 질병...  \n",
       "2  롯데그룹이 갈수록 치열해지는 온·오프라인 유통시장 경쟁에서 승리하기 위해 본격적인 ...  \n",
       "3  8년간 의식장애로 인한 마비로 침대에 누워있거나 휠체어에 묶여 있던 네덜란드 30대...  \n",
       "4  예상보다 글로벌 가입자가 적게 늘었다. 대신 한국과 일본이 체면치레를 해줬다. 넷플...  \n",
       "5  미국의 소행성 탐사선 '오시리스-렉스'가 20일 소행성 '101955 베누' 표면에...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({\"body\":[text1,text2,text3,text4,text5,text6],\"tgt\":[tgt_text1,tgt_text2,tgt_text3,tgt_text4,tgt_text5,tgt_text6]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:2].to_csv(\"train_data.csv\",index=False)\n",
    "df[2:4].to_csv(\"valid_data.csv\",index=False)\n",
    "df[4:6].to_csv(\"test_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "def load_data(row):\n",
    "    kkma=Kkma()\n",
    "    article=row[\"body\"]\n",
    "    abstract=row[\"tgt\"]\n",
    "#     article=article.text\n",
    "#     abstract=tgt_text\n",
    "    src_txt=kkma.sentences(article)\n",
    "    tgt_txt=kkma.sentences(abstract)\n",
    "\n",
    "    source=[{\"tokens\":kkma.morphs(clean(sen)),\"sentence\":clean(sen)} for sen in src_txt]\n",
    "    tgt=[{\"tokens\":kkma.morphs(clean(sen)),\"sentence\":clean(sen)} for sen in tgt_txt]\n",
    "    return source,tgt\n",
    "def _format_to_lines(f):\n",
    "    source, tgt = load_data(f)\n",
    "    return {'src': source, 'tgt': tgt}\n",
    "\n",
    "def format_to_lines():\n",
    "    for corpus_type in ['valid', 'test', 'train']:\n",
    "        df=pd.read_csv(corpus_type+\"_data.csv\")\n",
    "        df.apply(_format_to_lines,axis=1).to_json(corpus_type+\"_data.json\",orient=\"records\")\n",
    "format_to_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'src': [{'tokens': ['디지털',\n",
       "     '데일리',\n",
       "     '권',\n",
       "     '하영',\n",
       "     '기자',\n",
       "     '넷플릭스',\n",
       "     '가',\n",
       "     '기대',\n",
       "     '이하',\n",
       "     '성적',\n",
       "     '을',\n",
       "     '내',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': ' 디지털 데일리 권 하영 기자 넷플릭스가 기대 이하 성적을 냈다.'},\n",
       "   {'tokens': ['예상', '보다', '글로벌', '가입자', '가', '적', '게', '늘', '었', '다', '.'],\n",
       "    'sentence': '예상보다 글로벌 가입자가 적게 늘었다.'},\n",
       "   {'tokens': ['대신',\n",
       "     '한국',\n",
       "     '과',\n",
       "     '일본',\n",
       "     '이',\n",
       "     '체면',\n",
       "     '치레',\n",
       "     '를',\n",
       "     '하',\n",
       "     '어',\n",
       "     '주',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '대신 한국과 일본이 체면 치레를 해 줬다.'},\n",
       "   {'tokens': ['넷플릭스',\n",
       "     '가',\n",
       "     '특별히',\n",
       "     '언급',\n",
       "     '하',\n",
       "     '며',\n",
       "     '기뻐하',\n",
       "     'ㄹ',\n",
       "     '정도',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '넷플릭스가 특별히 언급하며 기뻐할 정도다.'},\n",
       "   {'tokens': ['국내',\n",
       "     '넷',\n",
       "     '플릭스',\n",
       "     '유료',\n",
       "     '가입자',\n",
       "     '는',\n",
       "     '330',\n",
       "     '만',\n",
       "     '명',\n",
       "     '으로',\n",
       "     '고공',\n",
       "     '행진',\n",
       "     '중이',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '국내 넷 플릭스 유료 가입자는 330만명으로 고공 행진 중이다.'},\n",
       "   {'tokens': ['일각',\n",
       "     '에서',\n",
       "     '는',\n",
       "     '곱',\n",
       "     '지',\n",
       "     '않',\n",
       "     '은',\n",
       "     '시선',\n",
       "     '도',\n",
       "     '보내',\n",
       "     'ㄴ다',\n",
       "     '.'],\n",
       "    'sentence': '일각에선 곱지 않은 시선도 보낸다.'},\n",
       "   {'tokens': ['최근',\n",
       "     '넷플릭스',\n",
       "     '는',\n",
       "     '조세',\n",
       "     '회피',\n",
       "     '혐의',\n",
       "     '로',\n",
       "     '국세청',\n",
       "     '세무',\n",
       "     '조사',\n",
       "     '를',\n",
       "     '받',\n",
       "     '은',\n",
       "     '데',\n",
       "     '이',\n",
       "     '다',\n",
       "     ','],\n",
       "    'sentence': '최근 넷플릭스는 조세 회피 혐의로 국세청 세무조사를 받은 데다,'},\n",
       "   {'tokens': ['국내',\n",
       "     '통신사',\n",
       "     '에',\n",
       "     '망',\n",
       "     '사용료',\n",
       "     '를',\n",
       "     '내',\n",
       "     '지',\n",
       "     '않',\n",
       "     '아',\n",
       "     '소송',\n",
       "     '까지',\n",
       "     '치르',\n",
       "     '고',\n",
       "     '있',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '국내 통신사에 망 사용료를 내지 않아 소송까지 치르고 있다.'},\n",
       "   {'tokens': ['여러모',\n",
       "     '로',\n",
       "     '무임승차',\n",
       "     '논란',\n",
       "     '을',\n",
       "     '빚',\n",
       "     '은',\n",
       "     '넷플릭스',\n",
       "     '가',\n",
       "     '한국',\n",
       "     '덕',\n",
       "     '을',\n",
       "     '보',\n",
       "     'ㄴ',\n",
       "     '것',\n",
       "     '이',\n",
       "     '아이러니',\n",
       "     '하다',\n",
       "     '는',\n",
       "     '지적',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.',\n",
       "     '20'],\n",
       "    'sentence': '여러모로 무임승차 논란을 빚은 넷플릭스가 한국 덕을 본 것이 아이러니 하다는 지적이다.20'},\n",
       "   {'tokens': ['일',\n",
       "     '현지',\n",
       "     '시각',\n",
       "     '넷플릭스',\n",
       "     '는',\n",
       "     '올해',\n",
       "     '3',\n",
       "     '분기',\n",
       "     '실적',\n",
       "     '발표',\n",
       "     '를',\n",
       "     '통하',\n",
       "     '어',\n",
       "     '글로벌',\n",
       "     '시장',\n",
       "     '에서',\n",
       "     '220',\n",
       "     '만',\n",
       "     '명',\n",
       "     '이',\n",
       "     '순증',\n",
       "     '하',\n",
       "     '어',\n",
       "     '유료',\n",
       "     '가입자',\n",
       "     '1',\n",
       "     '억',\n",
       "     '9500',\n",
       "     '만',\n",
       "     '명',\n",
       "     '을',\n",
       "     '돌파',\n",
       "     '하',\n",
       "     '었',\n",
       "     '다고'],\n",
       "    'sentence': '일 현지 시각 넷플릭스는 올해 3 분기 실적 발표를 통해 글로벌 시장에서 220만명이 순증해 유료 가입자 1억 9500만명을 돌파했다고'},\n",
       "   {'tokens': ['밝히', '었', '다', '.'], 'sentence': '밝혔다.'},\n",
       "   {'tokens': ['당초',\n",
       "     '시장',\n",
       "     '에서',\n",
       "     '는',\n",
       "     '357',\n",
       "     '만',\n",
       "     '명',\n",
       "     '이',\n",
       "     '순증',\n",
       "     '하',\n",
       "     'ㄹ',\n",
       "     '것',\n",
       "     '으로',\n",
       "     '전망',\n",
       "     '하',\n",
       "     '었',\n",
       "     '으나',\n",
       "     '이르',\n",
       "     'ㄹ',\n",
       "     '밑돌',\n",
       "     '는',\n",
       "     '성적표',\n",
       "     '를',\n",
       "     '받',\n",
       "     '았',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '당초 시장에서는 357만명이 순증할 것으로 전망했으나 이를 밑도는 성적표를 받았다.'},\n",
       "   {'tokens': ['지난해',\n",
       "     '같',\n",
       "     '은',\n",
       "     '기간',\n",
       "     '680',\n",
       "     '만',\n",
       "     '명',\n",
       "     '이',\n",
       "     '순증',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '것',\n",
       "     '과',\n",
       "     '비교',\n",
       "     '하',\n",
       "     '어도',\n",
       "     '크',\n",
       "     '게',\n",
       "     '주춤',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '모습',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '지난해 같은 기간 680만명이 순증한 것과 비교해도 크게 주춤 한 모습이다.'},\n",
       "   {'tokens': ['그나마',\n",
       "     '아시아',\n",
       "     '·',\n",
       "     '태평양',\n",
       "     '시장',\n",
       "     '에서',\n",
       "     '의',\n",
       "     '성과',\n",
       "     '가',\n",
       "     '크',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '그나마 아시아· 태평양 시장에서의 성과가 컸다.'},\n",
       "   {'tokens': ['이번',\n",
       "     '신규',\n",
       "     '가입자',\n",
       "     '가운데',\n",
       "     '이',\n",
       "     '지역',\n",
       "     '의',\n",
       "     '비중',\n",
       "     '은',\n",
       "     '46',\n",
       "     '%',\n",
       "     '에',\n",
       "     '달하',\n",
       "     'ㄴ다',\n",
       "     '.'],\n",
       "    'sentence': '이번 신규 가입자 가운데 이 지역의 비중은 46%에 달한다.'},\n",
       "   {'tokens': ['전년', '대비', '66', '%', '급증', '하', 'ㄴ', '규모', '이', '다', '.'],\n",
       "    'sentence': '전년 대비 66% 급증한 규모다.'},\n",
       "   {'tokens': ['특히',\n",
       "     '한국',\n",
       "     '과',\n",
       "     '일본',\n",
       "     '시장',\n",
       "     '에서',\n",
       "     '선전',\n",
       "     '하',\n",
       "     '고',\n",
       "     '있',\n",
       "     '는',\n",
       "     '것',\n",
       "     '으로',\n",
       "     '보이',\n",
       "     '어',\n",
       "     '지',\n",
       "     'ㄴ다',\n",
       "     '.'],\n",
       "    'sentence': '특히 한국과 일본 시장에서 선전하고 있는 것으로 보여 진다.'},\n",
       "   {'tokens': ['넷플릭스',\n",
       "     '는',\n",
       "     '이날',\n",
       "     '주주',\n",
       "     '들',\n",
       "     '에게',\n",
       "     '보내',\n",
       "     '는',\n",
       "     '서한',\n",
       "     '을',\n",
       "     '통하',\n",
       "     '어',\n",
       "     '“',\n",
       "     '한국',\n",
       "     '과',\n",
       "     '일본',\n",
       "     '에서',\n",
       "     '두',\n",
       "     '자릿수',\n",
       "     '점유율',\n",
       "     '을',\n",
       "     '달성',\n",
       "     '하',\n",
       "     '어',\n",
       "     '기쁘',\n",
       "     '다',\n",
       "     '”',\n",
       "     '고',\n",
       "     '언급',\n",
       "     '하',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '넷플릭스는 이날 주주들에게 보내는 서한을 통해 “ 한국과 일본에서 두 자릿수 점유율을 달성해 기쁘다” 고 언급했다.'},\n",
       "   {'tokens': ['로이',\n",
       "     '터',\n",
       "     '등',\n",
       "     '외신',\n",
       "     '은',\n",
       "     '넷플릭스',\n",
       "     '가',\n",
       "     '제작',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '‘',\n",
       "     '킹',\n",
       "     '덤',\n",
       "     '’',\n",
       "     '‘',\n",
       "     '인간',\n",
       "     '수업',\n",
       "     '’',\n",
       "     '‘',\n",
       "     '보건',\n",
       "     '교사',\n",
       "     '안',\n",
       "     '은',\n",
       "     '영',\n",
       "     '’',\n",
       "     '등',\n",
       "     '드라마',\n",
       "     '와',\n",
       "     '걸',\n",
       "     'ㄹ',\n",
       "     '그룹',\n",
       "     '블랙',\n",
       "     '핑크',\n",
       "     '의',\n",
       "     '다큐멘터리',\n",
       "     '와',\n",
       "     '같',\n",
       "     '은',\n",
       "     '한국',\n",
       "     '콘텐츠',\n",
       "     '가',\n",
       "     '넷플릭스',\n",
       "     '의',\n",
       "     '최대',\n",
       "     '성장',\n",
       "     '동력',\n",
       "     '중',\n",
       "     '하나',\n",
       "     '라고',\n",
       "     '지목',\n",
       "     '하',\n",
       "     '기',\n",
       "     '도',\n",
       "     '하',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '로이 터 등 외신은 넷플릭스가 제작한 ‘ 킹 덤’ ‘ 인간수업’ ‘ 보건교사 안은 영’ 등 드라마와 걸 그룹 블랙 핑크의 다큐멘터리와 같은 한국 콘텐츠가 넷플릭스의 최대 성장 동력 중 하나라고 지목하기도 했다.'},\n",
       "   {'tokens': ['넷플릭스',\n",
       "     '는',\n",
       "     '아시아',\n",
       "     '시장',\n",
       "     '에서',\n",
       "     '의',\n",
       "     '영향력',\n",
       "     '확대',\n",
       "     '를',\n",
       "     '위하',\n",
       "     '어',\n",
       "     '지나',\n",
       "     'ㄴ',\n",
       "     '2015',\n",
       "     '년',\n",
       "     '부터',\n",
       "     '한국',\n",
       "     '시장',\n",
       "     '에',\n",
       "     '약',\n",
       "     '7',\n",
       "     '억',\n",
       "     '달러',\n",
       "     '7973',\n",
       "     '억',\n",
       "     '원',\n",
       "     '규모',\n",
       "     '를',\n",
       "     '투자',\n",
       "     '하',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '넷플릭스는 아시아 시장에서의 영향력 확대를 위해 지난 2015년부터 한국 시장에 약 7억 달러 7973 억원 규모를 투자했다.'},\n",
       "   {'tokens': ['국내',\n",
       "     '넷',\n",
       "     '플릭스',\n",
       "     '유료',\n",
       "     '가입자',\n",
       "     '수',\n",
       "     '는',\n",
       "     '336',\n",
       "     '만',\n",
       "     '명',\n",
       "     '으로',\n",
       "     '추산',\n",
       "     '되',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '국내 넷 플릭스 유료 가입자 수는 336만명으로 추산됐다.'},\n",
       "   {'tokens': ['국내',\n",
       "     '유료',\n",
       "     '가입자',\n",
       "     '지표',\n",
       "     '가',\n",
       "     '공개',\n",
       "     '되',\n",
       "     'ㄴ',\n",
       "     '것',\n",
       "     '은',\n",
       "     '이번',\n",
       "     '이',\n",
       "     '처음',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '국내 유료 가입자 지표가 공개된 것은 이번이 처음이다.'},\n",
       "   {'tokens': ['작년',\n",
       "     '같',\n",
       "     '은',\n",
       "     '기간',\n",
       "     '184',\n",
       "     '만',\n",
       "     '명',\n",
       "     '보다',\n",
       "     '2',\n",
       "     '배',\n",
       "     '가까이',\n",
       "     '증가',\n",
       "     '하',\n",
       "     '었',\n",
       "     '고',\n",
       "     ',',\n",
       "     '국내',\n",
       "     '업체',\n",
       "     '중',\n",
       "     '최대',\n",
       "     'OTT',\n",
       "     '인',\n",
       "     '‘',\n",
       "     '웨이브',\n",
       "     '’',\n",
       "     '가입자',\n",
       "     '3',\n",
       "     '분기',\n",
       "     '기준',\n",
       "     '230',\n",
       "     '만',\n",
       "     '명',\n",
       "     '를',\n",
       "     '약',\n",
       "     '30',\n",
       "     '%',\n",
       "     '앞서',\n",
       "     '고',\n",
       "     '있',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '작년 같은 기간 184 만명 보다 2 배 가까이 증가했고, 국내 업체 중 최대 OTT 인 ‘ 웨이브’ 가입자 3 분기 기준 230만명 를 약 30% 앞서고 있다.'},\n",
       "   {'tokens': ['넷플릭스',\n",
       "     '가',\n",
       "     '이번',\n",
       "     '분기',\n",
       "     '국내',\n",
       "     '에서',\n",
       "     '상당',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '수익',\n",
       "     '을',\n",
       "     '벌',\n",
       "     '고',\n",
       "     '갈',\n",
       "     'ㄹ',\n",
       "     '것',\n",
       "     '으로',\n",
       "     '예상',\n",
       "     '되',\n",
       "     '면서',\n",
       "     '국회',\n",
       "     '를',\n",
       "     '중심',\n",
       "     '으로',\n",
       "     '한국',\n",
       "     '시장',\n",
       "     '무임승차',\n",
       "     '논란',\n",
       "     '이',\n",
       "     '다시',\n",
       "     '한번',\n",
       "     '불거지',\n",
       "     'ㄹ',\n",
       "     '전망',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '넷플릭스가 이번 분기 국내에서 상당한 수익을 벌고 갈 것으로 예상되면서 국회를 중심으로 한국시장 무임승차 논란이 다시 한번 불거질 전망이다.'},\n",
       "   {'tokens': ['과학',\n",
       "     '기술',\n",
       "     '정보',\n",
       "     '방송',\n",
       "     '통신',\n",
       "     '위원회',\n",
       "     '이하',\n",
       "     '과방',\n",
       "     '위',\n",
       "     '늘',\n",
       "     'ㄴ',\n",
       "     '오',\n",
       "     '는',\n",
       "     '23',\n",
       "     '일',\n",
       "     '국정',\n",
       "     '감사',\n",
       "     '증인',\n",
       "     '으로',\n",
       "     '레지',\n",
       "     '널드',\n",
       "     '숀',\n",
       "     '톰',\n",
       "     '슬',\n",
       "     'ㄴ',\n",
       "     '넷',\n",
       "     '플릭',\n",
       "     '스',\n",
       "     '서',\n",
       "     '비',\n",
       "     '시스',\n",
       "     '코리아',\n",
       "     '대표',\n",
       "     '를',\n",
       "     '부르',\n",
       "     '었',\n",
       "     '으나',\n",
       "     '해외',\n",
       "     '체류',\n",
       "     '중인',\n",
       "     '관계',\n",
       "     '로',\n",
       "     '불',\n",
       "     '출석',\n",
       "     '이',\n",
       "     '예상',\n",
       "     '되',\n",
       "     'ㄴ다',\n",
       "     '.'],\n",
       "    'sentence': '과학기술정보방송통신위원회 이하 과방 위 는 오는 23일 국정감사 증인으로 레지 널드 숀 톰 슨 넷 플릭 스서 비 시스 코리아 대표를 불렀으나 해외 체류 중인 관계로 불출석이 예상된다.'},\n",
       "   {'tokens': ['넷',\n",
       "     '플릭',\n",
       "     '슬',\n",
       "     '는',\n",
       "     'K',\n",
       "     '콘텐츠',\n",
       "     '를',\n",
       "     '무기',\n",
       "     '로',\n",
       "     '국내외',\n",
       "     '시장',\n",
       "     '에서',\n",
       "     '막대',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '돈',\n",
       "     '을',\n",
       "     '벌어들이',\n",
       "     '면서',\n",
       "     '도',\n",
       "     '정작',\n",
       "     '국내',\n",
       "     '에서',\n",
       "     '무임승차',\n",
       "     '만',\n",
       "     '하',\n",
       "     '고',\n",
       "     '있',\n",
       "     '다는',\n",
       "     '비판',\n",
       "     '을',\n",
       "     '받',\n",
       "     '고',\n",
       "     '있',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '넷 플릭 스는 K 콘텐츠를 무기로 국내외 시장에서 막대한 돈을 벌어들이면서도 정작 국내에서 무임승차만 하고 있다는 비판을 받고 있다.'},\n",
       "   {'tokens': ['특히',\n",
       "     '엄청나',\n",
       "     'ㄴ',\n",
       "     '트래픽',\n",
       "     '을',\n",
       "     '유발',\n",
       "     '하',\n",
       "     '면서',\n",
       "     '도',\n",
       "     '망',\n",
       "     '품질',\n",
       "     '유지',\n",
       "     '책임',\n",
       "     '은',\n",
       "     '외면',\n",
       "     '하',\n",
       "     'ㄴ다는',\n",
       "     '지적',\n",
       "     '이',\n",
       "     '적',\n",
       "     '지',\n",
       "     '않',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '특히 엄청난 트래픽을 유발하면서도 망 품질 유지 책임은 외면한다는 지적이 적지 않다.'},\n",
       "   {'tokens': ['국내',\n",
       "     '인터넷',\n",
       "     '제공',\n",
       "     '사업자',\n",
       "     'ISP',\n",
       "     '에',\n",
       "     '망',\n",
       "     '이용',\n",
       "     '대가',\n",
       "     '를',\n",
       "     '전혀',\n",
       "     '내',\n",
       "     '지',\n",
       "     '않',\n",
       "     '아',\n",
       "     'SK',\n",
       "     '브로드',\n",
       "     '밴드',\n",
       "     '와',\n",
       "     '는',\n",
       "     '소송',\n",
       "     '을',\n",
       "     '진행',\n",
       "     '중인',\n",
       "     '상황',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '국내 인터넷제공사업자 ISP 에 망 이용 대가를 전혀 내지 않아 SK 브로드 밴드와는 소송을 진행 중인 상황이다.'},\n",
       "   {'tokens': ['앞서',\n",
       "     '넷플릭스',\n",
       "     '는',\n",
       "     '조세',\n",
       "     '회피',\n",
       "     '혐의',\n",
       "     '로',\n",
       "     '국세청',\n",
       "     '세무',\n",
       "     '조사',\n",
       "     '대상',\n",
       "     '에',\n",
       "     '도',\n",
       "     '오르',\n",
       "     'ㄴ',\n",
       "     '바',\n",
       "     '있',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '앞서 넷플릭스는 조세 회피 혐의로 국세청 세무조사 대상에도 오른 바 있다.'},\n",
       "   {'tokens': ['박홍',\n",
       "     '근',\n",
       "     '더불',\n",
       "     '어',\n",
       "     '민주당',\n",
       "     '의원',\n",
       "     '이',\n",
       "     '국세청',\n",
       "     '으로',\n",
       "     '부터',\n",
       "     '받',\n",
       "     '은',\n",
       "     '자료',\n",
       "     '에',\n",
       "     '따르',\n",
       "     '면',\n",
       "     ',',\n",
       "     '넷플릭스',\n",
       "     '를',\n",
       "     '비롯',\n",
       "     '하',\n",
       "     '어',\n",
       "     '구',\n",
       "     '글',\n",
       "     '·',\n",
       "     '페이스',\n",
       "     '북',\n",
       "     '·',\n",
       "     '아마존',\n",
       "     '등',\n",
       "     '해외',\n",
       "     'IT',\n",
       "     '업체',\n",
       "     '134',\n",
       "     '개사',\n",
       "     '가',\n",
       "     '한',\n",
       "     '해',\n",
       "     '동안',\n",
       "     '납부',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '부가세',\n",
       "     '는',\n",
       "     '2367',\n",
       "     '억',\n",
       "     '원',\n",
       "     '에',\n",
       "     '불과',\n",
       "     '하다',\n",
       "     '.',\n",
       "     '네이버',\n",
       "     '1',\n",
       "     '개',\n",
       "     '사의',\n",
       "     '법인세',\n",
       "     '4500',\n",
       "     '억',\n",
       "     '원',\n",
       "     '보다',\n",
       "     '도',\n",
       "     '적',\n",
       "     '은',\n",
       "     '수준',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '박홍 근 더불어 민주당 의원이 국세청으로부터 받은 자료에 따르면, 넷플릭스를 비롯해 구 글· 페이스 북· 아마존 등 해외 IT 업체 134 개사가 한 해 동안 납부한 부가세는 2367억원에 불과 하다. 네이버 1개 사의 법인세 4500 억원 보다도 적은 수준이다.'},\n",
       "   {'tokens': ['한편',\n",
       "     ',',\n",
       "     '넷플릭스',\n",
       "     '의',\n",
       "     '3',\n",
       "     '분기',\n",
       "     '매출',\n",
       "     '은',\n",
       "     '64',\n",
       "     '억',\n",
       "     '4000',\n",
       "     '만',\n",
       "     '달러',\n",
       "     '약',\n",
       "     '7',\n",
       "     '조',\n",
       "     '3000',\n",
       "     '억',\n",
       "     '원',\n",
       "     '로',\n",
       "     '집계',\n",
       "     '되',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '한편, 넷플릭스의 3 분기 매출은 64억 4000만 달러 약 7조 3000억원 로 집계됐다.'},\n",
       "   {'tokens': ['전문가',\n",
       "     '전망',\n",
       "     '하',\n",
       "     '지',\n",
       "     '63',\n",
       "     '억',\n",
       "     '8',\n",
       "     '천만',\n",
       "     '달러',\n",
       "     '를',\n",
       "     '간신히',\n",
       "     '넘',\n",
       "     '은',\n",
       "     '수준',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '전문가 전망치 63 억 8천만 달러 를 간신히 넘은 수준이다.'},\n",
       "   {'tokens': ['주당',\n",
       "     '순이익',\n",
       "     'EPS',\n",
       "     '은',\n",
       "     '1.74',\n",
       "     '달러',\n",
       "     '로',\n",
       "     '예상치',\n",
       "     '2.14',\n",
       "     '달러',\n",
       "     '를',\n",
       "     '밑돌',\n",
       "     '았',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '주당 순이익 EPS 은 1.74 달러로 예상치 2.14 달러 를 밑돌았다.'},\n",
       "   {'tokens': ['예상',\n",
       "     '보다',\n",
       "     '저조',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '실적',\n",
       "     '이',\n",
       "     '나오',\n",
       "     '면서',\n",
       "     '이날',\n",
       "     '뉴욕',\n",
       "     '증시',\n",
       "     '에서',\n",
       "     '넷',\n",
       "     '플릭스',\n",
       "     '주가',\n",
       "     '는',\n",
       "     '시간외',\n",
       "     '거래',\n",
       "     '에서',\n",
       "     '최대',\n",
       "     '6',\n",
       "     '%',\n",
       "     '나',\n",
       "     '아',\n",
       "     '하락',\n",
       "     '하',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.',\n",
       "     'kwonhy'],\n",
       "    'sentence': '예상보다 저조한 실적이 나오면서 이날 뉴욕 증시에서 넷 플릭스 주가는 시간외 거래에서 최대 6% 나 하락했다.kwonhy'},\n",
       "   {'tokens': ['@', 'ddaily', '.', 'co', '.', 'kr'],\n",
       "    'sentence': '@ddaily .co .kr'}],\n",
       "  'tgt': [{'tokens': ['예상',\n",
       "     '보다',\n",
       "     '글로벌',\n",
       "     '가입자',\n",
       "     '가',\n",
       "     '적',\n",
       "     '게',\n",
       "     '늘',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '예상보다 글로벌 가입자가 적게 늘었다.'},\n",
       "   {'tokens': ['대신',\n",
       "     '한국',\n",
       "     '과',\n",
       "     '일본',\n",
       "     '이',\n",
       "     '체면',\n",
       "     '치레',\n",
       "     '를',\n",
       "     '하',\n",
       "     '어',\n",
       "     '주',\n",
       "     '었',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '대신 한국과 일본이 체면 치레를 해 줬다.'},\n",
       "   {'tokens': ['넷플릭스',\n",
       "     '가',\n",
       "     '이번',\n",
       "     '분기',\n",
       "     '국내',\n",
       "     '에서',\n",
       "     '상당',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '수익',\n",
       "     '을',\n",
       "     '벌',\n",
       "     '고',\n",
       "     '갈',\n",
       "     'ㄹ',\n",
       "     '것',\n",
       "     '으로',\n",
       "     '예상',\n",
       "     '되',\n",
       "     '면서',\n",
       "     '국회',\n",
       "     '를',\n",
       "     '중심',\n",
       "     '으로',\n",
       "     '한국',\n",
       "     '시장',\n",
       "     '무임승차',\n",
       "     '논란',\n",
       "     '이',\n",
       "     '다시',\n",
       "     '한번',\n",
       "     '불거지',\n",
       "     'ㄹ',\n",
       "     '전망',\n",
       "     '이',\n",
       "     '다',\n",
       "     '.'],\n",
       "    'sentence': '넷플릭스가 이번 분기 국내에서 상당한 수익을 벌고 갈 것으로 예상되면서 국회를 중심으로 한국시장 무임승차 논란이 다시 한번 불거질 전망이다.'}]},\n",
       " {'src': [{'tokens': ['사진',\n",
       "     '출처',\n",
       "     ':',\n",
       "     'NASA',\n",
       "     '사진',\n",
       "     '출처',\n",
       "     ':',\n",
       "     'NASA',\n",
       "     '사진',\n",
       "     '출처',\n",
       "     ':',\n",
       "     'NASA',\n",
       "     '사진',\n",
       "     '출처',\n",
       "     ':',\n",
       "     'NASA',\n",
       "     '미국',\n",
       "     '의',\n",
       "     '소행성',\n",
       "     '탐사선',\n",
       "     '오시',\n",
       "     '리스',\n",
       "     '-',\n",
       "     '렉스',\n",
       "     'Osiris-Rex',\n",
       "     '가',\n",
       "     '20',\n",
       "     '일',\n",
       "     '현지',\n",
       "     '시각',\n",
       "     '소행성',\n",
       "     '101955',\n",
       "     '베',\n",
       "     '누',\n",
       "     'Bennu',\n",
       "     '표면',\n",
       "     '에',\n",
       "     '접지',\n",
       "     '하',\n",
       "     '어',\n",
       "     '암석',\n",
       "     '표본',\n",
       "     '을',\n",
       "     '채집',\n",
       "     '하',\n",
       "     '는',\n",
       "     '데',\n",
       "     '성공',\n",
       "     '하',\n",
       "     '었',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '사진 출처: NASA 사진 출처: NASA 사진 출처: NASA 사진 출처: NASA 미국의 소행성 탐사선  오시 리스- 렉스 Osiris-Rex 가 20일 현지 시각 소행성 101955 베 누 Bennu 표면에 접지해 암석 표본을 채집하는 데 성공했습니다.'},\n",
       "   {'tokens': ['지구',\n",
       "     '에서',\n",
       "     '약',\n",
       "     '3',\n",
       "     '억',\n",
       "     '3,400',\n",
       "     '만',\n",
       "     'km',\n",
       "     '떨어지',\n",
       "     'ㄴ',\n",
       "     '곳',\n",
       "     '에서',\n",
       "     '전하',\n",
       "     '어',\n",
       "     '지',\n",
       "     'ㄴ',\n",
       "     '소식',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '지구에서 약 3억 3,400만 km 떨어진 곳에서 전해 진 소식입니다.'},\n",
       "   {'tokens': ['미',\n",
       "     '항공',\n",
       "     '우주국',\n",
       "     'NASA',\n",
       "     '은',\n",
       "     '4',\n",
       "     '년',\n",
       "     '전',\n",
       "     '오시',\n",
       "     '리스',\n",
       "     '렉스',\n",
       "     '를',\n",
       "     '발사',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '순간',\n",
       "     '부터',\n",
       "     '이',\n",
       "     '순간',\n",
       "     '을',\n",
       "     '기다리',\n",
       "     '며',\n",
       "     '매년',\n",
       "     '주기적',\n",
       "     '으로',\n",
       "     '관련',\n",
       "     '소식',\n",
       "     '을',\n",
       "     '업데이트',\n",
       "     '하',\n",
       "     '어',\n",
       "     '오',\n",
       "     '았',\n",
       "     '는데요',\n",
       "     '.',\n",
       "     '접지',\n",
       "     '하',\n",
       "     '는',\n",
       "     '순간',\n",
       "     '은',\n",
       "     '유',\n",
       "     '튜브',\n",
       "     '생',\n",
       "     '중계',\n",
       "     '를',\n",
       "     '통하',\n",
       "     '어',\n",
       "     '전',\n",
       "     '세계',\n",
       "     '에',\n",
       "     '생생',\n",
       "     '하',\n",
       "     '게',\n",
       "     '공개',\n",
       "     '되',\n",
       "     '었',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '미 항공 우주국 NASA 은 4년 전 오시 리스 렉스를 발사한 순간부터 이 순간을 기다리며 매년 주기적으로 관련 소식을 업데이트해 왔는데요. 접지하는 순간은 유 튜브 생중계를 통해 전 세계에 생생하게 공개됐습니다.'},\n",
       "   {'tokens': ['지름',\n",
       "     '500',\n",
       "     'm',\n",
       "     '가량',\n",
       "     '의',\n",
       "     '이',\n",
       "     '소행성',\n",
       "     '에서',\n",
       "     '토양',\n",
       "     '과',\n",
       "     '자갈',\n",
       "     '일부',\n",
       "     '60',\n",
       "     'g',\n",
       "     '가량',\n",
       "     '을',\n",
       "     '채취',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '것',\n",
       "     '이',\n",
       "     '왜',\n",
       "     '이토록',\n",
       "     '중요',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '걸',\n",
       "     'ㄹ까요',\n",
       "     '.'],\n",
       "    'sentence': '지름 500m 가량의 이 소행성에서 토양과 자갈 일부 60g 가량을 채취한 게 왜 이토록 중요한 걸까요.'},\n",
       "   {'tokens': ['태양계',\n",
       "     '에서',\n",
       "     '지금',\n",
       "     '까지',\n",
       "     '발견',\n",
       "     '되',\n",
       "     'ㄴ',\n",
       "     '소행성',\n",
       "     '은',\n",
       "     '약',\n",
       "     '78',\n",
       "     '만',\n",
       "     '개',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '태양계에서 지금까지 발견된 소행성은 약 78만 개입니다.'},\n",
       "   {'tokens': ['베',\n",
       "     '누',\n",
       "     '는',\n",
       "     '이',\n",
       "     '중',\n",
       "     '에서',\n",
       "     '도',\n",
       "     '좀',\n",
       "     '특별',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '소행성',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '베 누는 이 중에서도 좀 특별한 소행성입니다.'},\n",
       "   {'tokens': ['베',\n",
       "     '누',\n",
       "     '가',\n",
       "     '만들',\n",
       "     '어',\n",
       "     '지',\n",
       "     'ㄴ',\n",
       "     '시기',\n",
       "     '는',\n",
       "     '46',\n",
       "     '억',\n",
       "     '년',\n",
       "     '전',\n",
       "     '태양계',\n",
       "     '가',\n",
       "     '형성',\n",
       "     '되',\n",
       "     'ㄴ',\n",
       "     '후',\n",
       "     ',',\n",
       "     '1',\n",
       "     '천만',\n",
       "     '년',\n",
       "     '이',\n",
       "     '채',\n",
       "     '안',\n",
       "     '되',\n",
       "     'ㄴ',\n",
       "     '시기',\n",
       "     '로',\n",
       "     '추정',\n",
       "     '되',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '베 누가 만들어 진 시기는 46억 년 전 태양계가 형성된 후, 1천만 년이 채 안 된 시기로 추정됩니다.'},\n",
       "   {'tokens': ['이후',\n",
       "     '46',\n",
       "     '억',\n",
       "     '년',\n",
       "     '동안',\n",
       "     '의',\n",
       "     '태양계',\n",
       "     '형성',\n",
       "     '과정',\n",
       "     '에서',\n",
       "     '다른',\n",
       "     '행성',\n",
       "     '의',\n",
       "     '중력',\n",
       "     '과',\n",
       "     '소행성',\n",
       "     '과',\n",
       "     '의',\n",
       "     '충돌',\n",
       "     '등',\n",
       "     '에서',\n",
       "     '살아남',\n",
       "     '았',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '이후 46억 년 동안의 태양계 형성 과정에서 다른 행성의 중력과 소행성과의 충돌 등에서 살아남았습니다.'},\n",
       "   {'tokens': ['그동안',\n",
       "     '베',\n",
       "     '누',\n",
       "     '를',\n",
       "     '구성',\n",
       "     '하',\n",
       "     '는',\n",
       "     '물질',\n",
       "     '은',\n",
       "     '거의',\n",
       "     '변형',\n",
       "     '없이',\n",
       "     '그대로',\n",
       "     '간직',\n",
       "     '되',\n",
       "     'ㄴ',\n",
       "     '것',\n",
       "     '으로',\n",
       "     '여기',\n",
       "     '어',\n",
       "     '지',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '그동안 베 누를 구성하는 물질은 거의 변형 없이 그대로 간직된 것으로 여겨 집니다.'},\n",
       "   {'tokens': ['베',\n",
       "     '누의',\n",
       "     '샘플',\n",
       "     '을',\n",
       "     '채취',\n",
       "     '하',\n",
       "     '어',\n",
       "     '연구',\n",
       "     '하',\n",
       "     '면',\n",
       "     ',',\n",
       "     '초창기',\n",
       "     '태양계',\n",
       "     '형성',\n",
       "     '과',\n",
       "     '구성',\n",
       "     '성분',\n",
       "     ',',\n",
       "     '기원',\n",
       "     '을',\n",
       "     '알',\n",
       "     'ㄹ',\n",
       "     '길',\n",
       "     '이',\n",
       "     '열리',\n",
       "     'ㄹ',\n",
       "     '수',\n",
       "     '있',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '베 누의 샘플을 채취해 연구하면, 초창기 태양계 형성과 구성 성분, 기원을 알 길이 열릴 수 있습니다.'},\n",
       "   {'tokens': ['베',\n",
       "     '누',\n",
       "     '가',\n",
       "     '46',\n",
       "     '억',\n",
       "     '년',\n",
       "     '전',\n",
       "     '으로',\n",
       "     '날아가',\n",
       "     'ㄹ',\n",
       "     '수',\n",
       "     '있',\n",
       "     '는',\n",
       "     '타임',\n",
       "     '캡슐',\n",
       "     '역할',\n",
       "     '을',\n",
       "     '하',\n",
       "     '는',\n",
       "     '것',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '베 누가 46억 년 전으로 날아갈 수 있는  타임 캡슐 역할을 하는 겁니다.'},\n",
       "   {'tokens': ['이',\n",
       "     '와',\n",
       "     '관련',\n",
       "     '나사',\n",
       "     '는',\n",
       "     '베',\n",
       "     '누',\n",
       "     '를',\n",
       "     '가리키',\n",
       "     '어',\n",
       "     '지구',\n",
       "     '와',\n",
       "     '태양계',\n",
       "     '의',\n",
       "     '역사',\n",
       "     '를',\n",
       "     '말하',\n",
       "     '어',\n",
       "     '주',\n",
       "     'ㄹ',\n",
       "     '로제',\n",
       "     '타석',\n",
       "     '과',\n",
       "     '같',\n",
       "     '다',\n",
       "     '이',\n",
       "     '고',\n",
       "     '설명',\n",
       "     '하',\n",
       "     '었',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '이와 관련 나사는 베 누를 가리켜  지구와 태양계의 역사를 말해 줄 로제 타석과 같다 고 설명했습니다.'},\n",
       "   {'tokens': ['이렇',\n",
       "     '게',\n",
       "     '매력',\n",
       "     '적인',\n",
       "     '탐사',\n",
       "     '처',\n",
       "     '이',\n",
       "     'ㄴ',\n",
       "     '베',\n",
       "     '누',\n",
       "     '라',\n",
       "     '하',\n",
       "     '더라도',\n",
       "     ',',\n",
       "     '지구',\n",
       "     '에서',\n",
       "     '너무',\n",
       "     '멀리',\n",
       "     '떨어지',\n",
       "     '어',\n",
       "     '있',\n",
       "     '으면',\n",
       "     '탐사',\n",
       "     '하',\n",
       "     '기',\n",
       "     '어렵',\n",
       "     'ㄹ',\n",
       "     '것',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '이렇게 매력 적인 탐사 처인 베 누라 하더라도, 지구에서 너무 멀리 떨어져 있으면 탐사하기 어려울 겁니다.'},\n",
       "   {'tokens': ['혹은',\n",
       "     '자전',\n",
       "     '속도',\n",
       "     '가',\n",
       "     '너무',\n",
       "     '빨',\n",
       "     'ㄹ라',\n",
       "     '도',\n",
       "     '탐사선',\n",
       "     '이',\n",
       "     '내려앉',\n",
       "     '을',\n",
       "     '수',\n",
       "     '없',\n",
       "     '으니',\n",
       "     '곤란',\n",
       "     '하',\n",
       "     '게',\n",
       "     '되',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '혹은 자전 속도가 너무 빨라도 탐사선이 내려앉을 수 없으니 곤란하게 됩니다.'},\n",
       "   {'tokens': ['그런데',\n",
       "     '베',\n",
       "     '누의',\n",
       "     '조건',\n",
       "     '은',\n",
       "     '모든',\n",
       "     '면',\n",
       "     '에서',\n",
       "     '탐사',\n",
       "     '에',\n",
       "     '부합',\n",
       "     '하',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '그런데 베 누의 조건은 모든 면에서 탐사에 부합합니다.'},\n",
       "   {'tokens': ['우선',\n",
       "     '거리',\n",
       "     '면',\n",
       "     '에서',\n",
       "     ',',\n",
       "     '베',\n",
       "     '누',\n",
       "     '는',\n",
       "     '화성',\n",
       "     '과',\n",
       "     '목성',\n",
       "     '사이',\n",
       "     '에',\n",
       "     '위치',\n",
       "     '하',\n",
       "     '어',\n",
       "     '시속',\n",
       "     '10',\n",
       "     '만',\n",
       "     'km',\n",
       "     '의',\n",
       "     '속도',\n",
       "     '로',\n",
       "     '태양',\n",
       "     '궤도',\n",
       "     '를',\n",
       "     '공전',\n",
       "     '하',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '우선 거리 면에서, 베 누는 화성과 목성 사이에 위치해 시속 10만 km의 속도로 태양 궤도를 공전합니다.'},\n",
       "   {'tokens': ['6',\n",
       "     '년',\n",
       "     '마다',\n",
       "     '지구',\n",
       "     '와',\n",
       "     '가까워',\n",
       "     '지',\n",
       "     '는',\n",
       "     '지구',\n",
       "     '근접',\n",
       "     '천체',\n",
       "     '이',\n",
       "     'ㄴ데요',\n",
       "     '.'],\n",
       "    'sentence': '6년마다 지구와 가까워지는 지구 근접 천체인데요.'},\n",
       "   {'tokens': ['지구',\n",
       "     '근접',\n",
       "     '천체',\n",
       "     '는',\n",
       "     '지구',\n",
       "     '와',\n",
       "     '의',\n",
       "     '거리',\n",
       "     '가',\n",
       "     '특별히',\n",
       "     '가깝',\n",
       "     '어',\n",
       "     '우리',\n",
       "     '가',\n",
       "     '유심히',\n",
       "     '관찰',\n",
       "     '하',\n",
       "     '는',\n",
       "     '곳',\n",
       "     '들',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '지구 근접 천체는 지구와의 거리가 특별히 가까워 우리가 유심히 관찰하는 곳들입니다.'},\n",
       "   {'tokens': ['지금',\n",
       "     '베',\n",
       "     '누',\n",
       "     '는',\n",
       "     '3',\n",
       "     '억',\n",
       "     'km',\n",
       "     '가',\n",
       "     '넘',\n",
       "     '게',\n",
       "     '떨어지',\n",
       "     '어',\n",
       "     '있',\n",
       "     '지만',\n",
       "     ',',\n",
       "     '가깝',\n",
       "     'ㄹ',\n",
       "     '때',\n",
       "     '는',\n",
       "     '50',\n",
       "     '만',\n",
       "     'km',\n",
       "     '거리',\n",
       "     '까지',\n",
       "     '접근',\n",
       "     '하',\n",
       "     '기',\n",
       "     '도',\n",
       "     '하',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '지금 베 누는 3억 km가 넘게 떨어져 있지만, 가까울 때는 50만 km 거리까지 접근하기도 합니다.'},\n",
       "   {'tokens': ['이',\n",
       "     '정도',\n",
       "     '이',\n",
       "     '면',\n",
       "     '지구',\n",
       "     '에서',\n",
       "     '날아가',\n",
       "     'ㄴ',\n",
       "     '탐사선',\n",
       "     '이',\n",
       "     '샘플',\n",
       "     '을',\n",
       "     '채취',\n",
       "     '하',\n",
       "     '고',\n",
       "     '돌아오',\n",
       "     '기에',\n",
       "     '무리',\n",
       "     '없',\n",
       "     '는',\n",
       "     '거리',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '이 정도면 지구에서 날아간 탐사선이 샘플을 채취하고 돌아오기에 무리 없는 거리입니다.'},\n",
       "   {'tokens': ['자전', '속도', '도', '중요', '하', 'ㅂ니다', '.'],\n",
       "    'sentence': '자전 속도도 중요합니다.'},\n",
       "   {'tokens': ['소행성',\n",
       "     '이',\n",
       "     '한',\n",
       "     '바퀴',\n",
       "     '자전',\n",
       "     '하',\n",
       "     '는',\n",
       "     '데',\n",
       "     '1',\n",
       "     '분',\n",
       "     '이',\n",
       "     '걸리',\n",
       "     'ㄹ',\n",
       "     '정도',\n",
       "     '로',\n",
       "     '빠르',\n",
       "     '다면',\n",
       "     '탐사선',\n",
       "     '이',\n",
       "     '내려앉',\n",
       "     '아',\n",
       "     '표면',\n",
       "     '을',\n",
       "     '채취',\n",
       "     '하',\n",
       "     '기',\n",
       "     '어렵',\n",
       "     'ㄹ',\n",
       "     '수',\n",
       "     '있',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '소행성이 한 바퀴 자전하는 데 1분이 걸릴 정도로 빠르다면 탐사선이 내려앉아 표면을 채취하기 어려울 수 있습니다.'},\n",
       "   {'tokens': ['자전',\n",
       "     '속도',\n",
       "     '는',\n",
       "     '지름',\n",
       "     '크기',\n",
       "     '와',\n",
       "     '연관',\n",
       "     '되',\n",
       "     '는데',\n",
       "     '통상',\n",
       "     '지름',\n",
       "     '200',\n",
       "     'm',\n",
       "     '이내',\n",
       "     '의',\n",
       "     '소행',\n",
       "     '성은',\n",
       "     '착륙',\n",
       "     '이',\n",
       "     '어렵',\n",
       "     '다고'],\n",
       "    'sentence': '자전 속도는 지름 크기와 연관되는데 통상 지름 200m 이내의 소행 성은 착륙이 어렵다고'},\n",
       "   {'tokens': ['평가', '되', 'ㅂ니다', '.'], 'sentence': '평가 됩니다.'},\n",
       "   {'tokens': ['베',\n",
       "     '누',\n",
       "     '는',\n",
       "     '지름',\n",
       "     '492',\n",
       "     'm',\n",
       "     '로',\n",
       "     '자전',\n",
       "     '시간',\n",
       "     '이',\n",
       "     '4.3',\n",
       "     '시간',\n",
       "     '으로',\n",
       "     '탐사',\n",
       "     '에',\n",
       "     '용이',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '수준',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '베 누는 지름 492m 로 자전 시간이 4.3 시간으로 탐사에 용이한 수준입니다.'},\n",
       "   {'tokens': ['이번',\n",
       "     '에',\n",
       "     '채취',\n",
       "     '되',\n",
       "     'ㄴ',\n",
       "     '베',\n",
       "     '누의',\n",
       "     '샘플',\n",
       "     '은',\n",
       "     '2023',\n",
       "     '년',\n",
       "     '쯤',\n",
       "     '받',\n",
       "     '아',\n",
       "     '보',\n",
       "     'ㄹ',\n",
       "     '수',\n",
       "     '있',\n",
       "     '을',\n",
       "     '전망',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '이번에 채취된 베 누의 샘플은 2023년 쯤 받아 볼 수 있을 전망입니다.'},\n",
       "   {'tokens': ['나',\n",
       "     '아',\n",
       "     '살',\n",
       "     '는',\n",
       "     '이',\n",
       "     '를',\n",
       "     '통하',\n",
       "     '어',\n",
       "     '베',\n",
       "     '누의',\n",
       "     '구성',\n",
       "     '성분',\n",
       "     '과',\n",
       "     '기원',\n",
       "     ',',\n",
       "     '형성',\n",
       "     '과정',\n",
       "     '등',\n",
       "     '을',\n",
       "     '살피',\n",
       "     '고',\n",
       "     '수분',\n",
       "     '의',\n",
       "     '존재',\n",
       "     '여부',\n",
       "     '등',\n",
       "     '도',\n",
       "     '조사',\n",
       "     '하',\n",
       "     '게',\n",
       "     '되',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '나 사는 이를 통해 베 누의 구성 성분과 기원, 형성과정 등을 살피고 수분의 존재 여부 등도 조사하게 됩니다.'},\n",
       "   {'tokens': ['물',\n",
       "     '이나',\n",
       "     '얼음',\n",
       "     '이',\n",
       "     '있',\n",
       "     '다면',\n",
       "     '우주',\n",
       "     '어',\n",
       "     '어',\n",
       "     '디',\n",
       "     'ㄴ가',\n",
       "     '외계',\n",
       "     '생명체',\n",
       "     '의',\n",
       "     '존재',\n",
       "     '와',\n",
       "     '연결',\n",
       "     '되',\n",
       "     'ㄹ',\n",
       "     '수',\n",
       "     '있',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '물이나 얼음이 있다면 우주 어 딘가 외계 생명체의 존재와 연결될 수 있습니다.'},\n",
       "   {'tokens': ['특히',\n",
       "     '베',\n",
       "     '누',\n",
       "     '는',\n",
       "     '22',\n",
       "     '세기',\n",
       "     '말쯤',\n",
       "     '지구',\n",
       "     '와',\n",
       "     '충돌',\n",
       "     '하',\n",
       "     'ㄹ',\n",
       "     '확률',\n",
       "     '이',\n",
       "     '2,700',\n",
       "     '분',\n",
       "     '의',\n",
       "     '1',\n",
       "     '에',\n",
       "     '달하',\n",
       "     '는',\n",
       "     '것',\n",
       "     '으로',\n",
       "     '추정',\n",
       "     '되',\n",
       "     '는데요',\n",
       "     '.',\n",
       "     '나',\n",
       "     '살',\n",
       "     '는',\n",
       "     '이번',\n",
       "     '연구',\n",
       "     '를',\n",
       "     '통하',\n",
       "     '어',\n",
       "     '베',\n",
       "     '누의',\n",
       "     '궤도',\n",
       "     '를',\n",
       "     '바꾸',\n",
       "     'ㄹ',\n",
       "     '방안',\n",
       "     '도',\n",
       "     '연구',\n",
       "     '하',\n",
       "     'ㄹ',\n",
       "     '계획',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '특히 베 누는 22 세기 말쯤 지구와 충돌할 확률이 2,700분의 1에 달하는 것으로 추정되는데요. 나 사는 이번 연구를 통해 베 누의 궤도를 바꿀 방안도 연구할 계획입니다.'},\n",
       "   {'tokens': ['이번',\n",
       "     '에',\n",
       "     '오시',\n",
       "     '리스',\n",
       "     '-',\n",
       "     '렉스',\n",
       "     '가',\n",
       "     '베',\n",
       "     '누에',\n",
       "     '접지',\n",
       "     '하',\n",
       "     'ㄴ',\n",
       "     '시간',\n",
       "     '은',\n",
       "     '약',\n",
       "     '10',\n",
       "     '여',\n",
       "     '초',\n",
       "     '가량',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '이번에 오시 리스- 렉스가 베 누에 접지한 시간은 약 10여 초 가량입니다.'},\n",
       "   {'tokens': ['이르',\n",
       "     'ㄹ',\n",
       "     '위해',\n",
       "     '나사',\n",
       "     '는',\n",
       "     '예산',\n",
       "     '8',\n",
       "     '억',\n",
       "     '달러',\n",
       "     '약',\n",
       "     '9,050',\n",
       "     '억',\n",
       "     '원',\n",
       "     '를',\n",
       "     '들이',\n",
       "     '었',\n",
       "     '고',\n",
       "     ',',\n",
       "     '4',\n",
       "     '년',\n",
       "     '가까이',\n",
       "     '기다리',\n",
       "     '었',\n",
       "     '습니다',\n",
       "     '.',\n",
       "     '3'],\n",
       "    'sentence': '이를 위해 나사는 예산 8억 달러 약 9,050억 원 를 들였고, 4년 가까이 기다렸습니다.3'},\n",
       "   {'tokens': ['년',\n",
       "     '후',\n",
       "     '받',\n",
       "     '아',\n",
       "     '보',\n",
       "     '게',\n",
       "     '되',\n",
       "     'ㄹ',\n",
       "     '베',\n",
       "     '누의',\n",
       "     '표면',\n",
       "     '샘플',\n",
       "     '은',\n",
       "     '약',\n",
       "     '60',\n",
       "     'g',\n",
       "     '가량',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '년 후 받아 보게 될 베 누의 표면 샘플은 약 60g 가량입니다.'},\n",
       "   {'tokens': ['이르',\n",
       "     'ㄹ',\n",
       "     '통해',\n",
       "     '인류',\n",
       "     '는',\n",
       "     '우리',\n",
       "     '와',\n",
       "     '태양계',\n",
       "     '의',\n",
       "     '근원',\n",
       "     '을',\n",
       "     '추적',\n",
       "     '하',\n",
       "     '고',\n",
       "     '형성',\n",
       "     '과정',\n",
       "     '을',\n",
       "     '알아보',\n",
       "     'ㄹ',\n",
       "     '예정',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '이를 통해 인류는 우리와 태양계의 근원을 추적하고 형성 과정을 알아볼 예정입니다.'},\n",
       "   {'tokens': ['46',\n",
       "     '억',\n",
       "     '년',\n",
       "     '의',\n",
       "     '비밀',\n",
       "     '을',\n",
       "     '한',\n",
       "     '꺼풀',\n",
       "     '벗기',\n",
       "     '는',\n",
       "     '셈',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '46억 년의 비밀을 한 꺼풀 벗기는 셈입니다.'},\n",
       "   {'tokens': ['이승', '종', 'argo', '@', 'kbs', '.', 'co', '.', 'kr'],\n",
       "    'sentence': '이승 종 argo @kbs .co .kr'}],\n",
       "  'tgt': [{'tokens': ['미국',\n",
       "     '의',\n",
       "     '소행성',\n",
       "     '탐사선',\n",
       "     '오시',\n",
       "     '리스',\n",
       "     '-',\n",
       "     '렉스',\n",
       "     '가',\n",
       "     '아',\n",
       "     '20',\n",
       "     '일',\n",
       "     '소행성',\n",
       "     '101955',\n",
       "     '베',\n",
       "     '누',\n",
       "     '표면',\n",
       "     '에',\n",
       "     '접지',\n",
       "     '하',\n",
       "     '어',\n",
       "     '암석',\n",
       "     '표본',\n",
       "     '을',\n",
       "     '채집',\n",
       "     '하',\n",
       "     '는',\n",
       "     '데',\n",
       "     '성공',\n",
       "     '하',\n",
       "     '었',\n",
       "     '습니다',\n",
       "     '.'],\n",
       "    'sentence': '미국의 소행성 탐사선  오시 리스- 렉스 가 20일 소행성 101955 베 누 표면에 접지해 암석 표본을 채집하는 데 성공했습니다.'},\n",
       "   {'tokens': ['이번',\n",
       "     '에',\n",
       "     '채취',\n",
       "     '되',\n",
       "     'ㄴ',\n",
       "     '베',\n",
       "     '누의',\n",
       "     '샘플',\n",
       "     '은',\n",
       "     '2023',\n",
       "     '년',\n",
       "     '쯤',\n",
       "     '받',\n",
       "     '아',\n",
       "     '보',\n",
       "     'ㄹ',\n",
       "     '수',\n",
       "     '있',\n",
       "     '을',\n",
       "     '전망',\n",
       "     '이',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '이번에 채취된 베 누의 샘플은 2023년 쯤 받아 볼 수 있을 전망입니다.'},\n",
       "   {'tokens': ['나',\n",
       "     '아',\n",
       "     '살',\n",
       "     '는',\n",
       "     '이',\n",
       "     '를',\n",
       "     '통하',\n",
       "     '어',\n",
       "     '베',\n",
       "     '누의',\n",
       "     '구성',\n",
       "     '성분',\n",
       "     '과',\n",
       "     '기원',\n",
       "     ',',\n",
       "     '형성',\n",
       "     '과정',\n",
       "     '등',\n",
       "     '을',\n",
       "     '살피',\n",
       "     '고',\n",
       "     '수분',\n",
       "     '의',\n",
       "     '존재',\n",
       "     '여부',\n",
       "     '등',\n",
       "     '도',\n",
       "     '조사',\n",
       "     '하',\n",
       "     '게',\n",
       "     '되',\n",
       "     'ㅂ니다',\n",
       "     '.'],\n",
       "    'sentence': '나 사는 이를 통해 베 누의 구성 성분과 기원, 형성과정 등을 살피고 수분의 존재 여부 등도 조사하게 됩니다.'}]}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"test_data.json\",\"rb\") as f:\n",
    "    file=json.load(f)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "from os.path import join as pjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "def combination_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9가-힣 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    max_idx = (0, 0)\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    impossible_sents = []\n",
    "    for s in range(summary_size + 1):\n",
    "        combinations = itertools.combinations([i for i in range(len(sents)) if i not in impossible_sents], s + 1)\n",
    "        for c in combinations:\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if (s == 0 and rouge_score == 0):\n",
    "                impossible_sents.append(c[0])\n",
    "            if rouge_score > max_rouge:\n",
    "                max_idx = c\n",
    "                max_rouge = rouge_score\n",
    "    return sorted(list(max_idx))\n",
    "\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9가-힣 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return selected\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "\n",
    "    return sorted(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertData():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src, tgt, oracle_ids):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "        original_src_txt = list(map(lambda x:x[\"sentence\"],src))\n",
    "\n",
    "        labels = [0] * len(list(map(lambda x:x[\"tokens\"],src)))\n",
    "        for l in oracle_ids:\n",
    "            labels[l] = 1\n",
    "\n",
    "        if (len(labels) == 0):\n",
    "            return None\n",
    "\n",
    "        src_txt = original_src_txt\n",
    "        # text = [' '.join(ex['src_txt'][i].split()[:self.args.max_src_ntokens]) for i in idxs]\n",
    "        # text = [_clean(t) for t in text]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = labels[:len(cls_ids)]\n",
    "\n",
    "        tgt_txt = '<q>'.join(list(map(lambda x:x[\"sentence\"],tgt)))\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "\n",
    "\n",
    "def format_to_bert(oracle_mode): \n",
    "    for corpus_type in ['train', 'valid', 'test']:\n",
    "        _format_to_bert((corpus_type+\"_data.json\",oracle_mode, corpus_type+\"_data.bert.pt\"))\n",
    "\n",
    "\n",
    "def _format_to_bert(params):\n",
    "    json_file,oracle_mode,save_file = params\n",
    "#     if (os.path.exists(save_file)):\n",
    "#         logger.info('Ignore %s' % save_file)\n",
    "#         return\n",
    "\n",
    "    bert = BertData()\n",
    "\n",
    "#     logger.info('Processing %s' % json_file)\n",
    "    jobs = json.load(open(json_file))\n",
    "    datasets = []\n",
    "    for d in jobs:\n",
    "        source, tgt = d['src'], d['tgt']\n",
    "        source_tokens=list(map(lambda x:x[\"tokens\"],source))\n",
    "        tgt_tokens=list(map(lambda x:x[\"tokens\"],tgt))\n",
    "        if (oracle_mode == 'greedy'):\n",
    "            oracle_ids = greedy_selection(source_tokens, tgt_tokens, 3)\n",
    "            \n",
    "        elif (oracle_mode == 'combination'):\n",
    "            oracle_ids = combination_selection(source_tokens, tgt_tokens, 3)\n",
    "        b_data = bert.preprocess(source, tgt, oracle_ids)\n",
    "        if (b_data is None):\n",
    "            continue\n",
    "        indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        datasets.append(b_data_dict)\n",
    "#     logger.info('Saving to %s' % save_file)\n",
    "    torch.save(datasets, save_file)\n",
    "#     datasets = []\n",
    "#     gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_to_bert(\"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'src': [101,\n",
       "   1166,\n",
       "   32035,\n",
       "   12799,\n",
       "   87772,\n",
       "   44492,\n",
       "   21175,\n",
       "   13926,\n",
       "   1163,\n",
       "   97090,\n",
       "   11830,\n",
       "   49828,\n",
       "   13045,\n",
       "   59365,\n",
       "   90423,\n",
       "   18463,\n",
       "   28000,\n",
       "   69433,\n",
       "   44429,\n",
       "   11376,\n",
       "   1163,\n",
       "   32035,\n",
       "   14266,\n",
       "   12398,\n",
       "   35132,\n",
       "   52250,\n",
       "   76599,\n",
       "   58380,\n",
       "   17986,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1174,\n",
       "   61701,\n",
       "   15046,\n",
       "   74437,\n",
       "   13988,\n",
       "   50071,\n",
       "   66074,\n",
       "   20966,\n",
       "   62776,\n",
       "   37906,\n",
       "   1175,\n",
       "   33645,\n",
       "   12756,\n",
       "   15051,\n",
       "   1165,\n",
       "   63277,\n",
       "   20001,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   78845,\n",
       "   47529,\n",
       "   12211,\n",
       "   23488,\n",
       "   11112,\n",
       "   1177,\n",
       "   40389,\n",
       "   15463,\n",
       "   1177,\n",
       "   32035,\n",
       "   53890,\n",
       "   11643,\n",
       "   46957,\n",
       "   1175,\n",
       "   97090,\n",
       "   17986,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   90423,\n",
       "   18463,\n",
       "   28000,\n",
       "   69433,\n",
       "   44429,\n",
       "   11376,\n",
       "   1179,\n",
       "   49904,\n",
       "   12756,\n",
       "   59415,\n",
       "   18501,\n",
       "   1174,\n",
       "   46059,\n",
       "   37031,\n",
       "   22884,\n",
       "   1163,\n",
       "   32035,\n",
       "   97077,\n",
       "   33645,\n",
       "   15168,\n",
       "   81463,\n",
       "   12265,\n",
       "   12261,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   54602,\n",
       "   12756,\n",
       "   31516,\n",
       "   90423,\n",
       "   18463,\n",
       "   1180,\n",
       "   63277,\n",
       "   13926,\n",
       "   44429,\n",
       "   1174,\n",
       "   36581,\n",
       "   38259,\n",
       "   20966,\n",
       "   62776,\n",
       "   51431,\n",
       "   20269,\n",
       "   18823,\n",
       "   16961,\n",
       "   11945,\n",
       "   47468,\n",
       "   33705,\n",
       "   46957,\n",
       "   13045,\n",
       "   19218,\n",
       "   19810,\n",
       "   12362,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   12398,\n",
       "   12397,\n",
       "   64359,\n",
       "   11786,\n",
       "   11830,\n",
       "   47468,\n",
       "   17360,\n",
       "   12799,\n",
       "   53210,\n",
       "   33401,\n",
       "   19205,\n",
       "   12265,\n",
       "   1170,\n",
       "   29347,\n",
       "   72382,\n",
       "   12261,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1177,\n",
       "   31496,\n",
       "   48792,\n",
       "   90423,\n",
       "   18463,\n",
       "   28000,\n",
       "   69433,\n",
       "   44429,\n",
       "   11192,\n",
       "   1175,\n",
       "   29347,\n",
       "   25169,\n",
       "   1181,\n",
       "   31496,\n",
       "   90341,\n",
       "   1181,\n",
       "   46069,\n",
       "   35177,\n",
       "   11426,\n",
       "   54602,\n",
       "   12756,\n",
       "   25169,\n",
       "   39452,\n",
       "   47257,\n",
       "   32261,\n",
       "   20766,\n",
       "   32931,\n",
       "   70939,\n",
       "   44492,\n",
       "   12261,\n",
       "   117,\n",
       "   102,\n",
       "   101,\n",
       "   54602,\n",
       "   12756,\n",
       "   31516,\n",
       "   1179,\n",
       "   82890,\n",
       "   23918,\n",
       "   83133,\n",
       "   1169,\n",
       "   47328,\n",
       "   1172,\n",
       "   25539,\n",
       "   25125,\n",
       "   38259,\n",
       "   11643,\n",
       "   58380,\n",
       "   12799,\n",
       "   1174,\n",
       "   25539,\n",
       "   91011,\n",
       "   1172,\n",
       "   29347,\n",
       "   22585,\n",
       "   13045,\n",
       "   18721,\n",
       "   1177,\n",
       "   32035,\n",
       "   77129,\n",
       "   11724,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   30170,\n",
       "   38914,\n",
       "   11426,\n",
       "   1169,\n",
       "   46188,\n",
       "   41625,\n",
       "   66289,\n",
       "   23724,\n",
       "   1165,\n",
       "   59552,\n",
       "   38956,\n",
       "   10946,\n",
       "   1170,\n",
       "   32035,\n",
       "   73309,\n",
       "   90423,\n",
       "   18463,\n",
       "   28000,\n",
       "   69433,\n",
       "   44429,\n",
       "   11376,\n",
       "   47529,\n",
       "   23236,\n",
       "   23760,\n",
       "   76632,\n",
       "   27676,\n",
       "   1174,\n",
       "   47042,\n",
       "   41616,\n",
       "   25536,\n",
       "   76986,\n",
       "   11192,\n",
       "   1175,\n",
       "   32035,\n",
       "   15136,\n",
       "   12362,\n",
       "   119,\n",
       "   10200,\n",
       "   102,\n",
       "   101,\n",
       "   12398,\n",
       "   12397,\n",
       "   42703,\n",
       "   12799,\n",
       "   33401,\n",
       "   64359,\n",
       "   90423,\n",
       "   18463,\n",
       "   28000,\n",
       "   69433,\n",
       "   44429,\n",
       "   11192,\n",
       "   1174,\n",
       "   73057,\n",
       "   14840,\n",
       "   124,\n",
       "   1170,\n",
       "   45554,\n",
       "   13130,\n",
       "   33401,\n",
       "   12397,\n",
       "   15136,\n",
       "   1170,\n",
       "   82720,\n",
       "   37305,\n",
       "   11643,\n",
       "   25914,\n",
       "   13988,\n",
       "   50071,\n",
       "   66074,\n",
       "   33401,\n",
       "   83461,\n",
       "   12516,\n",
       "   15269,\n",
       "   18823,\n",
       "   63602,\n",
       "   13197,\n",
       "   11830,\n",
       "   24937,\n",
       "   13045,\n",
       "   14840,\n",
       "   1174,\n",
       "   36581,\n",
       "   38259,\n",
       "   20966,\n",
       "   62776,\n",
       "   13858,\n",
       "   122,\n",
       "   91057,\n",
       "   29499,\n",
       "   10995,\n",
       "   18823,\n",
       "   51281,\n",
       "   1166,\n",
       "   73057,\n",
       "   44849,\n",
       "   12771,\n",
       "   12300,\n",
       "   102,\n",
       "   101,\n",
       "   94194,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   84186,\n",
       "   13045,\n",
       "   55230,\n",
       "   33401,\n",
       "   83461,\n",
       "   36771,\n",
       "   30564,\n",
       "   18823,\n",
       "   63602,\n",
       "   13197,\n",
       "   11830,\n",
       "   24937,\n",
       "   13045,\n",
       "   15168,\n",
       "   24285,\n",
       "   27529,\n",
       "   85023,\n",
       "   14840,\n",
       "   21142,\n",
       "   35594,\n",
       "   1169,\n",
       "   32035,\n",
       "   97107,\n",
       "   56112,\n",
       "   52250,\n",
       "   15136,\n",
       "   37305,\n",
       "   11643,\n",
       "   45766,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1175,\n",
       "   32035,\n",
       "   29902,\n",
       "   14840,\n",
       "   18992,\n",
       "   1163,\n",
       "   32035,\n",
       "   19602,\n",
       "   34142,\n",
       "   18823,\n",
       "   63602,\n",
       "   13197,\n",
       "   11830,\n",
       "   24937,\n",
       "   71527,\n",
       "   40679,\n",
       "   12211,\n",
       "   1170,\n",
       "   32035,\n",
       "   25461,\n",
       "   14840,\n",
       "   12265,\n",
       "   60543,\n",
       "   36697,\n",
       "   97083,\n",
       "   46188,\n",
       "   13503,\n",
       "   17463,\n",
       "   1169,\n",
       "   29347,\n",
       "   13212,\n",
       "   84352,\n",
       "   12261,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   13988,\n",
       "   16801,\n",
       "   23823,\n",
       "   1174,\n",
       "   25539,\n",
       "   46022,\n",
       "   193,\n",
       "   1179,\n",
       "   26179,\n",
       "   97086,\n",
       "   42908,\n",
       "   36610,\n",
       "   33401,\n",
       "   83461,\n",
       "   12516,\n",
       "   10576,\n",
       "   52250,\n",
       "   12211,\n",
       "   11376,\n",
       "   1178,\n",
       "   33645,\n",
       "   17986,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   12398,\n",
       "   34910,\n",
       "   33401,\n",
       "   11830,\n",
       "   66482,\n",
       "   20966,\n",
       "   62776,\n",
       "   13858,\n",
       "   57237,\n",
       "   12398,\n",
       "   91787,\n",
       "   1170,\n",
       "   32035,\n",
       "   43312,\n",
       "   11375,\n",
       "   11703,\n",
       "   110,\n",
       "   17073,\n",
       "   84186,\n",
       "   86311,\n",
       "   12261,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   27529,\n",
       "   11084,\n",
       "   86735,\n",
       "   29234,\n",
       "   12393,\n",
       "   110,\n",
       "   13988,\n",
       "   17360,\n",
       "   24937,\n",
       "   71527,\n",
       "   1163,\n",
       "   36581,\n",
       "   38914,\n",
       "   12261,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   39392,\n",
       "   47529,\n",
       "   12211,\n",
       "   23488,\n",
       "   33401,\n",
       "   83461,\n",
       "   12516,\n",
       "   58957,\n",
       "   17101,\n",
       "   12727,\n",
       "   14126,\n",
       "   24285,\n",
       "   1170,\n",
       "   29347,\n",
       "   29961,\n",
       "   87550,\n",
       "   12261,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   90423,\n",
       "   18463,\n",
       "   28000,\n",
       "   69433,\n",
       "   44429,\n",
       "   11192,\n",
       "   12398,\n",
       "   41581,\n",
       "   36697,\n",
       "   18702,\n",
       "   60058,\n",
       "   1170,\n",
       "   29347,\n",
       "   82402,\n",
       "   1172,\n",
       "   33645,\n",
       "   11537,\n",
       "   10946,\n",
       "   25914,\n",
       "   100,\n",
       "   47529,\n",
       "   12211,\n",
       "   23488,\n",
       "   11786,\n",
       "   21494,\n",
       "   1175,\n",
       "   82754,\n",
       "   18463,\n",
       "   15783,\n",
       "   1175,\n",
       "   33645,\n",
       "   13503,\n",
       "   42159,\n",
       "   42159,\n",
       "   22058,\n",
       "   84186,\n",
       "   12397,\n",
       "   18856,\n",
       "   14840,\n",
       "   1163,\n",
       "   32035,\n",
       "   97077,\n",
       "   49904,\n",
       "   12261,\n",
       "   102],\n",
       "  'labels': [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'segs': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'clss': [0,\n",
       "   31,\n",
       "   51,\n",
       "   70,\n",
       "   96,\n",
       "   125,\n",
       "   144,\n",
       "   177,\n",
       "   208,\n",
       "   248,\n",
       "   308,\n",
       "   312,\n",
       "   344,\n",
       "   380,\n",
       "   405,\n",
       "   429,\n",
       "   446,\n",
       "   466],\n",
       "  'src_txt': [' 디지털 데일리 권 하영 기자 넷플릭스가 기대 이하 성적을 냈다.',\n",
       "   '예상보다 글로벌 가입자가 적게 늘었다.',\n",
       "   '대신 한국과 일본이 체면 치레를 해 줬다.',\n",
       "   '넷플릭스가 특별히 언급하며 기뻐할 정도다.',\n",
       "   '국내 넷 플릭스 유료 가입자는 330만명으로 고공 행진 중이다.',\n",
       "   '일각에선 곱지 않은 시선도 보낸다.',\n",
       "   '최근 넷플릭스는 조세 회피 혐의로 국세청 세무조사를 받은 데다,',\n",
       "   '국내 통신사에 망 사용료를 내지 않아 소송까지 치르고 있다.',\n",
       "   '여러모로 무임승차 논란을 빚은 넷플릭스가 한국 덕을 본 것이 아이러니 하다는 지적이다.20',\n",
       "   '일 현지 시각 넷플릭스는 올해 3 분기 실적 발표를 통해 글로벌 시장에서 220만명이 순증해 유료 가입자 1억 9500만명을 돌파했다고',\n",
       "   '밝혔다.',\n",
       "   '당초 시장에서는 357만명이 순증할 것으로 전망했으나 이를 밑도는 성적표를 받았다.',\n",
       "   '지난해 같은 기간 680만명이 순증한 것과 비교해도 크게 주춤 한 모습이다.',\n",
       "   '그나마 아시아· 태평양 시장에서의 성과가 컸다.',\n",
       "   '이번 신규 가입자 가운데 이 지역의 비중은 46%에 달한다.',\n",
       "   '전년 대비 66% 급증한 규모다.',\n",
       "   '특히 한국과 일본 시장에서 선전하고 있는 것으로 보여 진다.',\n",
       "   '넷플릭스는 이날 주주들에게 보내는 서한을 통해 “ 한국과 일본에서 두 자릿수 점유율을 달성해 기쁘다” 고 언급했다.',\n",
       "   '로이 터 등 외신은 넷플릭스가 제작한 ‘ 킹 덤’ ‘ 인간수업’ ‘ 보건교사 안은 영’ 등 드라마와 걸 그룹 블랙 핑크의 다큐멘터리와 같은 한국 콘텐츠가 넷플릭스의 최대 성장 동력 중 하나라고 지목하기도 했다.',\n",
       "   '넷플릭스는 아시아 시장에서의 영향력 확대를 위해 지난 2015년부터 한국 시장에 약 7억 달러 7973 억원 규모를 투자했다.',\n",
       "   '국내 넷 플릭스 유료 가입자 수는 336만명으로 추산됐다.',\n",
       "   '국내 유료 가입자 지표가 공개된 것은 이번이 처음이다.',\n",
       "   '작년 같은 기간 184 만명 보다 2 배 가까이 증가했고, 국내 업체 중 최대 OTT 인 ‘ 웨이브’ 가입자 3 분기 기준 230만명 를 약 30% 앞서고 있다.',\n",
       "   '넷플릭스가 이번 분기 국내에서 상당한 수익을 벌고 갈 것으로 예상되면서 국회를 중심으로 한국시장 무임승차 논란이 다시 한번 불거질 전망이다.',\n",
       "   '과학기술정보방송통신위원회 이하 과방 위 는 오는 23일 국정감사 증인으로 레지 널드 숀 톰 슨 넷 플릭 스서 비 시스 코리아 대표를 불렀으나 해외 체류 중인 관계로 불출석이 예상된다.',\n",
       "   '넷 플릭 스는 K 콘텐츠를 무기로 국내외 시장에서 막대한 돈을 벌어들이면서도 정작 국내에서 무임승차만 하고 있다는 비판을 받고 있다.',\n",
       "   '특히 엄청난 트래픽을 유발하면서도 망 품질 유지 책임은 외면한다는 지적이 적지 않다.',\n",
       "   '국내 인터넷제공사업자 ISP 에 망 이용 대가를 전혀 내지 않아 SK 브로드 밴드와는 소송을 진행 중인 상황이다.',\n",
       "   '앞서 넷플릭스는 조세 회피 혐의로 국세청 세무조사 대상에도 오른 바 있다.',\n",
       "   '박홍 근 더불어 민주당 의원이 국세청으로부터 받은 자료에 따르면, 넷플릭스를 비롯해 구 글· 페이스 북· 아마존 등 해외 IT 업체 134 개사가 한 해 동안 납부한 부가세는 2367억원에 불과 하다. 네이버 1개 사의 법인세 4500 억원 보다도 적은 수준이다.',\n",
       "   '한편, 넷플릭스의 3 분기 매출은 64억 4000만 달러 약 7조 3000억원 로 집계됐다.',\n",
       "   '전문가 전망치 63 억 8천만 달러 를 간신히 넘은 수준이다.',\n",
       "   '주당 순이익 EPS 은 1.74 달러로 예상치 2.14 달러 를 밑돌았다.',\n",
       "   '예상보다 저조한 실적이 나오면서 이날 뉴욕 증시에서 넷 플릭스 주가는 시간외 거래에서 최대 6% 나 하락했다.kwonhy',\n",
       "   '@ddaily .co .kr'],\n",
       "  'tgt_txt': '예상보다 글로벌 가입자가 적게 늘었다.<q>대신 한국과 일본이 체면 치레를 해 줬다.<q>넷플릭스가 이번 분기 국내에서 상당한 수익을 벌고 갈 것으로 예상되면서 국회를 중심으로 한국시장 무임승차 논란이 다시 한번 불거질 전망이다.'},\n",
       " {'src': [101,\n",
       "   1172,\n",
       "   25539,\n",
       "   19218,\n",
       "   1177,\n",
       "   65633,\n",
       "   58137,\n",
       "   131,\n",
       "   10156,\n",
       "   1172,\n",
       "   25539,\n",
       "   19218,\n",
       "   1177,\n",
       "   65633,\n",
       "   58137,\n",
       "   131,\n",
       "   10156,\n",
       "   1172,\n",
       "   25539,\n",
       "   19218,\n",
       "   1177,\n",
       "   65633,\n",
       "   58137,\n",
       "   131,\n",
       "   10156,\n",
       "   1172,\n",
       "   25539,\n",
       "   19218,\n",
       "   1177,\n",
       "   65633,\n",
       "   58137,\n",
       "   131,\n",
       "   10156,\n",
       "   37379,\n",
       "   1172,\n",
       "   29347,\n",
       "   26617,\n",
       "   18856,\n",
       "   1179,\n",
       "   54691,\n",
       "   13551,\n",
       "   19205,\n",
       "   1174,\n",
       "   29347,\n",
       "   14280,\n",
       "   1168,\n",
       "   32035,\n",
       "   13212,\n",
       "   118,\n",
       "   1168,\n",
       "   40389,\n",
       "   44429,\n",
       "   85374,\n",
       "   118,\n",
       "   24832,\n",
       "   20966,\n",
       "   41124,\n",
       "   42703,\n",
       "   12799,\n",
       "   33401,\n",
       "   64359,\n",
       "   1172,\n",
       "   29347,\n",
       "   26617,\n",
       "   18856,\n",
       "   14334,\n",
       "   11518,\n",
       "   89520,\n",
       "   1170,\n",
       "   40389,\n",
       "   1165,\n",
       "   46188,\n",
       "   34574,\n",
       "   10136,\n",
       "   1180,\n",
       "   44840,\n",
       "   15463,\n",
       "   10609,\n",
       "   1175,\n",
       "   55588,\n",
       "   12799,\n",
       "   14840,\n",
       "   1174,\n",
       "   54691,\n",
       "   40482,\n",
       "   1180,\n",
       "   44840,\n",
       "   55030,\n",
       "   10946,\n",
       "   1177,\n",
       "   26179,\n",
       "   38653,\n",
       "   12508,\n",
       "   44492,\n",
       "   52250,\n",
       "   33705,\n",
       "   14840,\n",
       "   97104,\n",
       "   13212,\n",
       "   79427,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1175,\n",
       "   32035,\n",
       "   16336,\n",
       "   11786,\n",
       "   33293,\n",
       "   124,\n",
       "   91057,\n",
       "   124,\n",
       "   117,\n",
       "   11036,\n",
       "   18823,\n",
       "   10166,\n",
       "   1167,\n",
       "   90609,\n",
       "   47468,\n",
       "   91460,\n",
       "   27529,\n",
       "   14840,\n",
       "   87550,\n",
       "   1172,\n",
       "   29347,\n",
       "   21400,\n",
       "   62776,\n",
       "   82566,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1169,\n",
       "   32035,\n",
       "   1181,\n",
       "   47328,\n",
       "   33705,\n",
       "   1174,\n",
       "   46188,\n",
       "   18702,\n",
       "   20687,\n",
       "   10156,\n",
       "   14494,\n",
       "   125,\n",
       "   11084,\n",
       "   27529,\n",
       "   1174,\n",
       "   29347,\n",
       "   14280,\n",
       "   1168,\n",
       "   32035,\n",
       "   13212,\n",
       "   1168,\n",
       "   40389,\n",
       "   44429,\n",
       "   11643,\n",
       "   1170,\n",
       "   82720,\n",
       "   13551,\n",
       "   11537,\n",
       "   13197,\n",
       "   11830,\n",
       "   19602,\n",
       "   18139,\n",
       "   12398,\n",
       "   13197,\n",
       "   11830,\n",
       "   86308,\n",
       "   1163,\n",
       "   32035,\n",
       "   12261,\n",
       "   13926,\n",
       "   19724,\n",
       "   1169,\n",
       "   26179,\n",
       "   11084,\n",
       "   36697,\n",
       "   13130,\n",
       "   16744,\n",
       "   81828,\n",
       "   1172,\n",
       "   29347,\n",
       "   47175,\n",
       "   1174,\n",
       "   55588,\n",
       "   26872,\n",
       "   56937,\n",
       "   14840,\n",
       "   27607,\n",
       "   32414,\n",
       "   47024,\n",
       "   119,\n",
       "   1175,\n",
       "   55588,\n",
       "   12799,\n",
       "   12508,\n",
       "   13197,\n",
       "   11830,\n",
       "   19602,\n",
       "   11375,\n",
       "   1174,\n",
       "   36581,\n",
       "   1179,\n",
       "   36581,\n",
       "   30555,\n",
       "   88678,\n",
       "   89388,\n",
       "   13045,\n",
       "   52441,\n",
       "   25914,\n",
       "   27529,\n",
       "   32572,\n",
       "   10609,\n",
       "   88678,\n",
       "   13045,\n",
       "   24247,\n",
       "   17855,\n",
       "   47468,\n",
       "   13045,\n",
       "   22123,\n",
       "   97072,\n",
       "   83955,\n",
       "   97104,\n",
       "   13212,\n",
       "   79427,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1175,\n",
       "   32035,\n",
       "   48121,\n",
       "   10755,\n",
       "   10150,\n",
       "   20966,\n",
       "   43620,\n",
       "   10576,\n",
       "   12398,\n",
       "   1172,\n",
       "   29347,\n",
       "   26617,\n",
       "   18856,\n",
       "   11786,\n",
       "   1179,\n",
       "   29347,\n",
       "   36610,\n",
       "   12211,\n",
       "   1175,\n",
       "   25539,\n",
       "   95281,\n",
       "   46931,\n",
       "   10780,\n",
       "   10251,\n",
       "   20966,\n",
       "   43620,\n",
       "   10946,\n",
       "   1177,\n",
       "   26179,\n",
       "   97083,\n",
       "   38365,\n",
       "   11537,\n",
       "   1163,\n",
       "   40389,\n",
       "   1174,\n",
       "   83955,\n",
       "   12398,\n",
       "   26569,\n",
       "   46412,\n",
       "   61377,\n",
       "   1163,\n",
       "   84098,\n",
       "   97070,\n",
       "   25539,\n",
       "   47024,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1179,\n",
       "   26179,\n",
       "   36610,\n",
       "   21892,\n",
       "   11786,\n",
       "   1175,\n",
       "   32035,\n",
       "   39504,\n",
       "   18721,\n",
       "   1170,\n",
       "   82720,\n",
       "   83677,\n",
       "   11830,\n",
       "   13721,\n",
       "   1172,\n",
       "   29347,\n",
       "   26617,\n",
       "   18856,\n",
       "   11375,\n",
       "   33293,\n",
       "   12637,\n",
       "   18823,\n",
       "   1163,\n",
       "   26179,\n",
       "   62776,\n",
       "   82566,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1170,\n",
       "   40389,\n",
       "   1165,\n",
       "   46188,\n",
       "   11192,\n",
       "   12398,\n",
       "   19810,\n",
       "   86329,\n",
       "   1175,\n",
       "   50823,\n",
       "   1179,\n",
       "   49904,\n",
       "   12756,\n",
       "   59415,\n",
       "   11537,\n",
       "   1172,\n",
       "   29347,\n",
       "   26617,\n",
       "   53196,\n",
       "   79427,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1170,\n",
       "   40389,\n",
       "   1165,\n",
       "   46188,\n",
       "   11376,\n",
       "   1169,\n",
       "   26646,\n",
       "   88223,\n",
       "   87550,\n",
       "   33401,\n",
       "   46687,\n",
       "   11703,\n",
       "   91057,\n",
       "   1165,\n",
       "   43107,\n",
       "   27529,\n",
       "   1179,\n",
       "   26179,\n",
       "   36610,\n",
       "   21892,\n",
       "   11376,\n",
       "   1181,\n",
       "   42908,\n",
       "   80835,\n",
       "   20902,\n",
       "   117,\n",
       "   122,\n",
       "   38028,\n",
       "   18823,\n",
       "   1165,\n",
       "   43107,\n",
       "   11112,\n",
       "   1177,\n",
       "   26179,\n",
       "   1174,\n",
       "   26646,\n",
       "   36915,\n",
       "   33401,\n",
       "   62569,\n",
       "   1177,\n",
       "   46188,\n",
       "   17861,\n",
       "   97072,\n",
       "   31496,\n",
       "   79427,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   18778,\n",
       "   11703,\n",
       "   91057,\n",
       "   1165,\n",
       "   43107,\n",
       "   41417,\n",
       "   10576,\n",
       "   1179,\n",
       "   26179,\n",
       "   36610,\n",
       "   21892,\n",
       "   1181,\n",
       "   42908,\n",
       "   18856,\n",
       "   30482,\n",
       "   82093,\n",
       "   20066,\n",
       "   46957,\n",
       "   13045,\n",
       "   82725,\n",
       "   19810,\n",
       "   28120,\n",
       "   12211,\n",
       "   1172,\n",
       "   29347,\n",
       "   26617,\n",
       "   18856,\n",
       "   96093,\n",
       "   1177,\n",
       "   73072,\n",
       "   12265,\n",
       "   12397,\n",
       "   74061,\n",
       "   12516,\n",
       "   1172,\n",
       "   82720,\n",
       "   18253,\n",
       "   39061,\n",
       "   18253,\n",
       "   97104,\n",
       "   13212,\n",
       "   79427,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   13988,\n",
       "   88619,\n",
       "   1170,\n",
       "   40389,\n",
       "   1165,\n",
       "   46188,\n",
       "   11643,\n",
       "   54602,\n",
       "   18856,\n",
       "   12508,\n",
       "   1169,\n",
       "   65633,\n",
       "   50502,\n",
       "   11375,\n",
       "   53689,\n",
       "   1170,\n",
       "   43107,\n",
       "   27423,\n",
       "   70531,\n",
       "   13988,\n",
       "   30642,\n",
       "   20966,\n",
       "   11830,\n",
       "   40419,\n",
       "   13721,\n",
       "   24285,\n",
       "   1174,\n",
       "   46069,\n",
       "   83677,\n",
       "   1175,\n",
       "   32035,\n",
       "   79427,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1170,\n",
       "   40389,\n",
       "   1165,\n",
       "   46188,\n",
       "   10576,\n",
       "   88678,\n",
       "   13503,\n",
       "   28000,\n",
       "   22058,\n",
       "   1177,\n",
       "   26179,\n",
       "   97083,\n",
       "   38365,\n",
       "   14840,\n",
       "   87400,\n",
       "   37693,\n",
       "   117,\n",
       "   1177,\n",
       "   29347,\n",
       "   94115,\n",
       "   13130,\n",
       "   1179,\n",
       "   26179,\n",
       "   36610,\n",
       "   21892,\n",
       "   1181,\n",
       "   42908,\n",
       "   18856,\n",
       "   12211,\n",
       "   54602,\n",
       "   18856,\n",
       "   52250,\n",
       "   31049,\n",
       "   117,\n",
       "   1163,\n",
       "   32035,\n",
       "   49943,\n",
       "   74793,\n",
       "   1163,\n",
       "   70910,\n",
       "   11112,\n",
       "   1174,\n",
       "   59817,\n",
       "   13926,\n",
       "   12397,\n",
       "   13197,\n",
       "   12398,\n",
       "   97104,\n",
       "   13212,\n",
       "   79427,\n",
       "   119,\n",
       "   102,\n",
       "   101,\n",
       "   1170,\n",
       "   40389,\n",
       "   1165,\n",
       "   46188,\n",
       "   102],\n",
       "  'labels': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'segs': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'clss': [0, 102, 129, 225, 273, 302, 325, 373, 418, 453, 506],\n",
       "  'src_txt': ['사진 출처: NASA 사진 출처: NASA 사진 출처: NASA 사진 출처: NASA 미국의 소행성 탐사선  오시 리스- 렉스 Osiris-Rex 가 20일 현지 시각 소행성 101955 베 누 Bennu 표면에 접지해 암석 표본을 채집하는 데 성공했습니다.',\n",
       "   '지구에서 약 3억 3,400만 km 떨어진 곳에서 전해 진 소식입니다.',\n",
       "   '미 항공 우주국 NASA 은 4년 전 오시 리스 렉스를 발사한 순간부터 이 순간을 기다리며 매년 주기적으로 관련 소식을 업데이트해 왔는데요. 접지하는 순간은 유 튜브 생중계를 통해 전 세계에 생생하게 공개됐습니다.',\n",
       "   '지름 500m 가량의 이 소행성에서 토양과 자갈 일부 60g 가량을 채취한 게 왜 이토록 중요한 걸까요.',\n",
       "   '태양계에서 지금까지 발견된 소행성은 약 78만 개입니다.',\n",
       "   '베 누는 이 중에서도 좀 특별한 소행성입니다.',\n",
       "   '베 누가 만들어 진 시기는 46억 년 전 태양계가 형성된 후, 1천만 년이 채 안 된 시기로 추정됩니다.',\n",
       "   '이후 46억 년 동안의 태양계 형성 과정에서 다른 행성의 중력과 소행성과의 충돌 등에서 살아남았습니다.',\n",
       "   '그동안 베 누를 구성하는 물질은 거의 변형 없이 그대로 간직된 것으로 여겨 집니다.',\n",
       "   '베 누의 샘플을 채취해 연구하면, 초창기 태양계 형성과 구성 성분, 기원을 알 길이 열릴 수 있습니다.',\n",
       "   '베 누가 46억 년 전으로 날아갈 수 있는  타임 캡슐 역할을 하는 겁니다.',\n",
       "   '이와 관련 나사는 베 누를 가리켜  지구와 태양계의 역사를 말해 줄 로제 타석과 같다 고 설명했습니다.',\n",
       "   '이렇게 매력 적인 탐사 처인 베 누라 하더라도, 지구에서 너무 멀리 떨어져 있으면 탐사하기 어려울 겁니다.',\n",
       "   '혹은 자전 속도가 너무 빨라도 탐사선이 내려앉을 수 없으니 곤란하게 됩니다.',\n",
       "   '그런데 베 누의 조건은 모든 면에서 탐사에 부합합니다.',\n",
       "   '우선 거리 면에서, 베 누는 화성과 목성 사이에 위치해 시속 10만 km의 속도로 태양 궤도를 공전합니다.',\n",
       "   '6년마다 지구와 가까워지는 지구 근접 천체인데요.',\n",
       "   '지구 근접 천체는 지구와의 거리가 특별히 가까워 우리가 유심히 관찰하는 곳들입니다.',\n",
       "   '지금 베 누는 3억 km가 넘게 떨어져 있지만, 가까울 때는 50만 km 거리까지 접근하기도 합니다.',\n",
       "   '이 정도면 지구에서 날아간 탐사선이 샘플을 채취하고 돌아오기에 무리 없는 거리입니다.',\n",
       "   '자전 속도도 중요합니다.',\n",
       "   '소행성이 한 바퀴 자전하는 데 1분이 걸릴 정도로 빠르다면 탐사선이 내려앉아 표면을 채취하기 어려울 수 있습니다.',\n",
       "   '자전 속도는 지름 크기와 연관되는데 통상 지름 200m 이내의 소행 성은 착륙이 어렵다고',\n",
       "   '평가 됩니다.',\n",
       "   '베 누는 지름 492m 로 자전 시간이 4.3 시간으로 탐사에 용이한 수준입니다.',\n",
       "   '이번에 채취된 베 누의 샘플은 2023년 쯤 받아 볼 수 있을 전망입니다.',\n",
       "   '나 사는 이를 통해 베 누의 구성 성분과 기원, 형성과정 등을 살피고 수분의 존재 여부 등도 조사하게 됩니다.',\n",
       "   '물이나 얼음이 있다면 우주 어 딘가 외계 생명체의 존재와 연결될 수 있습니다.',\n",
       "   '특히 베 누는 22 세기 말쯤 지구와 충돌할 확률이 2,700분의 1에 달하는 것으로 추정되는데요. 나 사는 이번 연구를 통해 베 누의 궤도를 바꿀 방안도 연구할 계획입니다.',\n",
       "   '이번에 오시 리스- 렉스가 베 누에 접지한 시간은 약 10여 초 가량입니다.',\n",
       "   '이를 위해 나사는 예산 8억 달러 약 9,050억 원 를 들였고, 4년 가까이 기다렸습니다.3',\n",
       "   '년 후 받아 보게 될 베 누의 표면 샘플은 약 60g 가량입니다.',\n",
       "   '이를 통해 인류는 우리와 태양계의 근원을 추적하고 형성 과정을 알아볼 예정입니다.',\n",
       "   '46억 년의 비밀을 한 꺼풀 벗기는 셈입니다.',\n",
       "   '이승 종 argo @kbs .co .kr'],\n",
       "  'tgt_txt': '미국의 소행성 탐사선  오시 리스- 렉스 가 20일 소행성 101955 베 누 표면에 접지해 암석 표본을 채집하는 데 성공했습니다.<q>이번에 채취된 베 누의 샘플은 2023년 쯤 받아 볼 수 있을 전망입니다.<q>나 사는 이를 통해 베 누의 구성 성분과 기원, 형성과정 등을 살피고 수분의 존재 여부 등도 조사하게 됩니다.'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=torch.load(\"test_data.bert.pt\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
